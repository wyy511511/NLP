{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 2: Language Models and Neural Networks\n",
    "#### CSCI 3832 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your name and email here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this homework we're going to be looking at the bigram language model you've implemented in class, and extend it to trigrams.\n",
    "\n",
    "Instead of looking at the Bible, we'll re-visit the sentiment analysis problem from the previous homework. This dataset contains a split of unlabeled movie reviews. We'll train our language model using this unlabeled split (i.e. we'll pretrain our language model) and then we'll use this model as a starting off point for a neural classification model (i.e. finetuning), which we'll use to do sentiment classification. Finally, we'll replace our trained embeddings with Glove pretrained vectors, to see if we get any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Neural Language Modeling\n",
    "\n",
    "We'll first load the unsupervised data. Set the data dir below to the directory you used for Homework 1, to prevent copying the data twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.13.0\n",
      "  latest version: 23.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, sys, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, numpy\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A newspaperman (Johnny Twennies) living in the 90's with a complete 20's personality and lifestyle - fedora, manual typewriter, the Charleston, the works. It's a great idea for a movie and it couldn't have been done better.<br /><br />Johnny doesn't miss a cliche, but never uses the same one twice. You'll find yourself anticipating his reactions to the harsher '90s world as the movie goes along, you'll often guess right - but that makes the movie just that much more fun.<br /><br />Lots of fun when Johnny is called on to save the same damsel in distress (named Virginia, natch) on three different occasions. She responds with appropriate fluttering eyelids each time.<br /><br />His reaction to independent women, openly gay men, and the general '90s milieu is delightful. He remains happily oblivious.<br /><br />Don't worry, the movie never takes itself seriously. Nobody preaches about the evil of the present, or the shallowness of the past. You end up with a warm feeling for all the characters, even the bad guys. This was one of those rare movies where you can actually feel that the performers are thoroughly enjoying their characters. The film makers make sure you know that with a delightfully offbeat ending.<br /><br />\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/a511/Desktop/cu23 spring/3832/aclImdb/'\n",
    "data_limit = 15000\n",
    "\n",
    "def read_folder(folder):\n",
    "    examples = []\n",
    "    for fname in os.listdir(folder)[:data_limit]:\n",
    "        with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "            examples.append(f.readline().strip())\n",
    "    return examples\n",
    "\n",
    "unsup_examples = read_folder(os.path.join(data_dir, 'train/unsup/'))\n",
    "print(unsup_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset also comes with a pre-made vocabulary, which we'll rely on for this section of the homework. We'll eventually convert our words to indices, so lets store the words in a dictionary, mapping each to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_file = os.path.join(data_dir, 'imdb.vocab')\n",
    "\n",
    "raw_vocabulary = []\n",
    "with open(vocabulary_file, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        raw_vocabulary.append(line.strip())\n",
    "\n",
    "#Limit our vocabulary size to top 5k words\n",
    "raw_vocabulary = raw_vocabulary[:5000]\n",
    "\n",
    "# Add in our special tokens\n",
    "special_tokens = ['<s>', '</s>', '<unk>']\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Create the vocabulary dictionary by prepending the special tokens to the raw vocabulary, and enumerating them.\n",
    "\n",
    "10 pts.\n",
    "\n",
    "'''\n",
    "vocabulary_list = special_tokens + raw_vocabulary\n",
    "vocabulary = {word: index for index, word in enumerate(vocabulary_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(vocabulary, dict)\n",
    "assert len(vocabulary) == 5003\n",
    "assert vocabulary['<s>'] == 0\n",
    "assert vocabulary['significance'] == 5002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<s>': 0, '</s>': 1, '<unk>': 2, 'the': 3, 'and': 4, 'a': 5, 'of': 6, 'to': 7, 'is': 8, 'it': 9, 'in': 10, 'i': 11, 'this': 12, 'that': 13, 'was': 14, 'as': 15, 'for': 16, 'with': 17, 'movie': 18, 'but': 19, 'film': 20, 'on': 21, 'not': 22, 'you': 23, 'he': 24, 'are': 25, 'his': 26, 'have': 27, 'be': 28, 'one': 29, '!': 30, 'all': 31, 'at': 32, 'by': 33, 'an': 34, 'who': 35, 'they': 36, 'from': 37, 'so': 38, 'like': 39, 'there': 40, 'her': 41, 'or': 42, 'just': 43, 'about': 44, 'if': 45, 'has': 46, 'out': 47, 'what': 48, '?': 49, 'some': 50, 'good': 51, 'more': 52, 'when': 53, 'she': 54, 'very': 55, 'even': 56, 'my': 57, 'no': 58, 'up': 59, 'time': 60, 'would': 61, 'which': 62, 'only': 63, 'story': 64, 'really': 65, 'their': 66, 'see': 67, 'had': 68, 'can': 69, 'were': 70, 'me': 71, 'we': 72, 'than': 73, 'well': 74, 'much': 75, 'been': 76, 'get': 77, 'people': 78, 'will': 79, 'bad': 80, 'other': 81, 'also': 82, 'into': 83, 'do': 84, 'because': 85, 'great': 86, 'first': 87, 'how': 88, 'him': 89, 'most': 90, \"don't\": 91, 'its': 92, 'made': 93, 'then': 94, 'them': 95, 'way': 96, 'make': 97, 'could': 98, 'too': 99, 'any': 100, 'after': 101, 'movies': 102, 'think': 103, 'characters': 104, 'character': 105, 'watch': 106, 'films': 107, 'two': 108, 'many': 109, 'seen': 110, 'being': 111, 'acting': 112, 'never': 113, 'plot': 114, 'little': 115, 'where': 116, 'love': 117, 'best': 118, 'life': 119, 'did': 120, 'show': 121, 'know': 122, 'does': 123, 'ever': 124, 'here': 125, 'better': 126, 'man': 127, 'your': 128, 'still': 129, 'end': 130, 'over': 131, 'off': 132, 'these': 133, 'say': 134, 'scene': 135, 'why': 136, 'while': 137, 'scenes': 138, 'such': 139, 'go': 140, 'something': 141, 'should': 142, 'through': 143, 'back': 144, \"i'm\": 145, 'those': 146, 'watching': 147, 'real': 148, 'though': 149, 'now': 150, \"doesn't\": 151, 'thing': 152, 'years': 153, 'actors': 154, 'director': 155, 'another': 156, \"didn't\": 157, 'before': 158, 'nothing': 159, 'new': 160, 'funny': 161, 'actually': 162, 'work': 163, 'makes': 164, 'find': 165, 'look': 166, 'old': 167, 'few': 168, 'going': 169, 'same': 170, 'again': 171, 'lot': 172, 'part': 173, 'every': 174, 'cast': 175, 'us': 176, 'world': 177, 'quite': 178, 'want': 179, 'things': 180, 'pretty': 181, 'young': 182, 'seems': 183, 'around': 184, 'got': 185, 'down': 186, 'however': 187, \"can't\": 188, 'fact': 189, 'horror': 190, 'take': 191, 'enough': 192, 'both': 193, 'may': 194, 'give': 195, 'original': 196, 'between': 197, 'big': 198, 'own': 199, \"i've\": 200, 'thought': 201, 'series': 202, 'without': 203, 'right': 204, 'times': 205, 'long': 206, 'always': 207, 'gets': 208, 'action': 209, 'comedy': 210, \"isn't\": 211, 'family': 212, 'come': 213, 'point': 214, 'role': 215, 'saw': 216, 'interesting': 217, 'almost': 218, 'least': 219, 'whole': 220, 'must': 221, 'bit': 222, 'music': 223, 'script': 224, 'done': 225, 'guy': 226, 'anything': 227, 'minutes': 228, 'last': 229, 'since': 230, 'might': 231, 'performance': 232, 'far': 233, 'feel': 234, 'probably': 235, 'am': 236, 'woman': 237, 'kind': 238, 'girl': 239, 'away': 240, 'yet': 241, 'rather': 242, 'worst': 243, 'sure': 244, 'fun': 245, 'anyone': 246, 'making': 247, 'each': 248, 'played': 249, 'tv': 250, 'found': 251, 'having': 252, 'day': 253, 'although': 254, 'especially': 255, 'our': 256, 'course': 257, 'believe': 258, 'comes': 259, 'trying': 260, 'goes': 261, 'hard': 262, 'looks': 263, 'different': 264, 'place': 265, 'book': 266, 'actor': 267, 'put': 268, 'maybe': 269, 'money': 270, 'someone': 271, 'ending': 272, 'reason': 273, 'let': 274, 'everything': 275, \"wasn't\": 276, 'sense': 277, 'once': 278, 'shows': 279, 'screen': 280, 'dvd': 281, 'true': 282, 'set': 283, 'worth': 284, 'job': 285, 'main': 286, 'looking': 287, 'watched': 288, 'everyone': 289, 'together': 290, 'three': 291, 'plays': 292, 'john': 293, 'later': 294, 'said': 295, 'play': 296, 'instead': 297, 'audience': 298, 'seem': 299, 'beautiful': 300, 'takes': 301, 'effects': 302, 'himself': 303, 'version': 304, 'during': 305, 'left': 306, 'night': 307, 'house': 308, 'seeing': 309, 'wife': 310, 'special': 311, 'excellent': 312, 'father': 313, 'american': 314, 'idea': 315, 'else': 316, 'nice': 317, 'shot': 318, 'simply': 319, 'year': 320, 'read': 321, \"you're\": 322, 'black': 323, 'high': 324, 'less': 325, 'war': 326, 'star': 327, 'completely': 328, 'help': 329, 'fan': 330, 'poor': 331, 'death': 332, 'second': 333, 'men': 334, 'hollywood': 335, 'either': 336, 'mind': 337, 'used': 338, 'given': 339, 'home': 340, 'kids': 341, 'try': 342, 'performances': 343, 'women': 344, 'enjoy': 345, 'classic': 346, 'boring': 347, 'short': 348, 'wrong': 349, 'need': 350, 'rest': 351, 'use': 352, 'friends': 353, 'until': 354, 'along': 355, 'dead': 356, 'truly': 357, 'half': 358, 'production': 359, 'line': 360, 'tell': 361, 'couple': 362, 'remember': 363, 'next': 364, 'start': 365, 'stupid': 366, 'perhaps': 367, 'came': 368, 'recommend': 369, 'moments': 370, 'awful': 371, 'wonderful': 372, 'episode': 373, 'mean': 374, 'understand': 375, 'terrible': 376, 'full': 377, 'camera': 378, 'getting': 379, 'stars': 380, 'playing': 381, 'video': 382, 'keep': 383, 'sex': 384, 'doing': 385, 'others': 386, 'early': 387, 'often': 388, 'small': 389, 'definitely': 390, 'gives': 391, 'person': 392, 'school': 393, 'face': 394, 'perfect': 395, 'name': 396, 'itself': 397, 'become': 398, 'human': 399, 'lines': 400, 'yes': 401, 'finally': 402, 'dialogue': 403, 'lost': 404, 'felt': 405, 'case': 406, 'piece': 407, 'top': 408, 'liked': 409, 'supposed': 410, 'children': 411, \"couldn't\": 412, 'title': 413, 'absolutely': 414, 'head': 415, 'budget': 416, 'mother': 417, 'live': 418, 'written': 419, 'boy': 420, 'picture': 421, 'against': 422, 'cinema': 423, 'went': 424, 'worse': 425, 'certainly': 426, 'entire': 427, 'sort': 428, 'style': 429, 'waste': 430, 'problem': 431, 'mr': 432, 'hope': 433, 'entertaining': 434, 'overall': 435, 'friend': 436, 'killer': 437, 'evil': 438, 'several': 439, 'loved': 440, 'fans': 441, 'oh': 442, 'beginning': 443, 'white': 444, 'lives': 445, 'care': 446, 'becomes': 447, 'direction': 448, 'example': 449, 'already': 450, 'based': 451, 'drama': 452, 'despite': 453, 'seemed': 454, 'dark': 455, 'throughout': 456, 'unfortunately': 457, 'wanted': 458, \"i'd\": 459, 'final': 460, 'history': 461, 'amazing': 462, 'turn': 463, 'fine': 464, 'laugh': 465, 'michael': 466, 'son': 467, 'humor': 468, 'guess': 469, 'totally': 470, 'lead': 471, 'sound': 472, 'guys': 473, 'writing': 474, \"you'll\": 475, 'wants': 476, 'low': 477, 'works': 478, 'tries': 479, 'called': 480, 'under': 481, 'past': 482, 'viewer': 483, 'quality': 484, 'child': 485, 'days': 486, 'behind': 487, 'game': 488, 'turns': 489, \"they're\": 490, 'enjoyed': 491, 'today': 492, 'able': 493, 'act': 494, 'town': 495, 'favorite': 496, 'kill': 497, 'flick': 498, 'starts': 499, 'gave': 500, 'actress': 501, 'sometimes': 502, 'eyes': 503, 'etc': 504, 'side': 505, 'horrible': 506, 'girls': 507, 'genre': 508, 'soon': 509, \"won't\": 510, 'car': 511, 'brilliant': 512, 'parts': 513, 'art': 514, 'heart': 515, 'themselves': 516, 'expect': 517, 'kid': 518, 'stuff': 519, 'stories': 520, 'thinking': 521, 'city': 522, 'obviously': 523, 'directed': 524, 'late': 525, 'myself': 526, 'blood': 527, 'decent': 528, 'feeling': 529, 'run': 530, 's': 531, 'highly': 532, 'god': 533, 'except': 534, 'close': 535, 'fight': 536, 'hand': 537, 'anyway': 538, 'roles': 539, 'daughter': 540, 'killed': 541, 'moment': 542, 'says': 543, 'heard': 544, 'leave': 545, 'matter': 546, 'took': 547, 'cannot': 548, 'police': 549, 'happens': 550, 'brother': 551, 'hour': 552, 'violence': 553, 'happened': 554, 'strong': 555, 'particularly': 556, 'james': 557, 'extremely': 558, 'involved': 559, 'chance': 560, 'writer': 561, 'obvious': 562, 'experience': 563, \"wouldn't\": 564, 'lack': 565, 'including': 566, 'told': 567, 'alone': 568, 'attempt': 569, 'murder': 570, 'living': 571, 'happen': 572, 'please': 573, 'age': 574, 'wonder': 575, 'complete': 576, 'ago': 577, 'voice': 578, 'group': 579, 'score': 580, 'david': 581, 'coming': 582, 'interest': 583, 'save': 584, 'none': 585, 'ok': 586, 'crap': 587, 'type': 588, 'looked': 589, 'simple': 590, 'slow': 591, 'possible': 592, 'number': 593, 'seriously': 594, 'hell': 595, 'gore': 596, 'exactly': 597, 'shown': 598, 'king': 599, 'hero': 600, 'annoying': 601, 'song': 602, 'husband': 603, 'sad': 604, 'whose': 605, 'career': 606, 'yourself': 607, 'cinematography': 608, 'taken': 609, 'musical': 610, 'ends': 611, 'usually': 612, 'serious': 613, \"i'll\": 614, 'stop': 615, 'hours': 616, 'scary': 617, 'released': 618, 'across': 619, 'hilarious': 620, 'running': 621, 'reality': 622, 'relationship': 623, 'usual': 624, 'ridiculous': 625, 'known': 626, 'hit': 627, 'opening': 628, 'somewhat': 629, 'started': 630, 'opinion': 631, 'jokes': 632, 'novel': 633, 'cool': 634, 'change': 635, 'robert': 636, 'wish': 637, 'ones': 638, 'body': 639, 'finds': 640, 'order': 641, 'saying': 642, 'english': 643, 'huge': 644, 'cut': 645, 'shots': 646, 'episodes': 647, 'mostly': 648, 'taking': 649, 'female': 650, 'talking': 651, 'strange': 652, 'major': 653, 'view': 654, 'power': 655, 'happy': 656, 'documentary': 657, 'apparently': 658, 'rating': 659, 'disappointed': 660, 'level': 661, 'talent': 662, 'call': 663, 'country': 664, 'jack': 665, 'due': 666, 'events': 667, 'room': 668, 'important': 669, 'songs': 670, 'basically': 671, 'clearly': 672, 'knows': 673, 'knew': 674, 'supporting': 675, 'attention': 676, 'television': 677, 'future': 678, 'turned': 679, 'paul': 680, 'easily': 681, 'problems': 682, \"aren't\": 683, 'silly': 684, 'british': 685, 'word': 686, 'tells': 687, 'earth': 688, 'words': 689, 'local': 690, 'single': 691, 'light': 692, 'four': 693, 'cheap': 694, 'sequence': 695, 'bring': 696, 'entertainment': 697, 'thriller': 698, 'beyond': 699, 'george': 700, 'miss': 701, 'modern': 702, 'whether': 703, 'b': 704, 'predictable': 705, 'falls': 706, 'five': 707, 'sets': 708, 'similar': 709, 'review': 710, 'richard': 711, 'needs': 712, 'upon': 713, 'lady': 714, 'enjoyable': 715, 'appears': 716, 'romantic': 717, 'comic': 718, 'giving': 719, 'mystery': 720, 'talk': 721, 'rock': 722, 'message': 723, 'animation': 724, 'within': 725, 'theater': 726, 'sequel': 727, 'bunch': 728, 'mention': 729, 'herself': 730, 'feels': 731, 'nearly': 732, 'points': 733, 'theme': 734, 'lee': 735, 'above': 736, 'dull': 737, 'add': 738, 'york': 739, 'ways': 740, 'moving': 741, 'storyline': 742, 'ten': 743, 'surprised': 744, \"haven't\": 745, 'lots': 746, 'team': 747, 'begins': 748, 'middle': 749, 'using': 750, 'fantastic': 751, 'actual': 752, 'sister': 753, 'effort': 754, 'viewers': 755, 'named': 756, 'among': 757, 'elements': 758, 'easy': 759, 'stay': 760, 'tom': 761, 'comments': 762, 'avoid': 763, 'showing': 764, 'typical': 765, 'release': 766, 'clear': 767, 'hate': 768, 'tried': 769, 'peter': 770, 'sorry': 771, 'french': 772, 'dialog': 773, 'editing': 774, 'tale': 775, 'certain': 776, 'season': 777, 'soundtrack': 778, 'buy': 779, 'fall': 780, 'general': 781, 'near': 782, 'means': 783, 'famous': 784, 'check': 785, 'parents': 786, 'red': 787, 'somehow': 788, 'material': 789, 'oscar': 790, 'period': 791, 'form': 792, 'straight': 793, 'weak': 794, 'working': 795, 'doubt': 796, 'class': 797, 'leads': 798, 'filmed': 799, 'gone': 800, 'kept': 801, 'greatest': 802, 'figure': 803, 'viewing': 804, 'feature': 805, 'disney': 806, 'eye': 807, 'realistic': 808, 'brought': 809, 'imagine': 810, 'hear': 811, 'atmosphere': 812, 'fast': 813, 'particular': 814, 'suspense': 815, 'lame': 816, 'move': 817, 'whatever': 818, 'sequences': 819, 'america': 820, 'follow': 821, 'indeed': 822, 'die': 823, 'crime': 824, 'learn': 825, 'eventually': 826, 'reviews': 827, 'wait': 828, 'forget': 829, 'deal': 830, 'zombie': 831, 'dance': 832, 'space': 833, 'okay': 834, 'premise': 835, 'surprise': 836, 'believable': 837, 'nature': 838, 'possibly': 839, \"you've\": 840, 'third': 841, 'decided': 842, 'subject': 843, 'expected': 844, 'de': 845, 'japanese': 846, 'dr': 847, 'became': 848, 'truth': 849, 'imdb': 850, 'sexual': 851, 'average': 852, 'stand': 853, 'difficult': 854, 'screenplay': 855, 'romance': 856, 't': 857, 'sit': 858, 'poorly': 859, 'rent': 860, 'joe': 861, 'nor': 862, 'leaves': 863, 'question': 864, 'stage': 865, 'note': 866, 'begin': 867, 'killing': 868, 'needed': 869, 'reading': 870, 'unless': 871, 'baby': 872, 'superb': 873, 'directors': 874, 'society': 875, 'otherwise': 876, 'shame': 877, 'meet': 878, 'street': 879, 'situation': 880, 'meets': 881, 'memorable': 882, 'dog': 883, 'credits': 884, 'earlier': 885, 'forced': 886, 'weird': 887, 'minute': 888, 'older': 889, 'laughs': 890, 'realize': 891, 'emotional': 892, 'jane': 893, 'beauty': 894, 'writers': 895, 'comment': 896, 'footage': 897, 'write': 898, 'ask': 899, 'badly': 900, 'interested': 901, 'dramatic': 902, 'sounds': 903, 'whom': 904, 'hot': 905, 'keeps': 906, 'features': 907, 'directing': 908, 'mess': 909, 'development': 910, 'crazy': 911, 'quickly': 912, 'male': 913, 'mark': 914, 'towards': 915, 'creepy': 916, 'free': 917, 'monster': 918, 'perfectly': 919, 'result': 920, 'total': 921, 'plus': 922, 'previous': 923, 'brings': 924, 'unique': 925, 'plenty': 926, 'worked': 927, 'cheesy': 928, 'effect': 929, 'personal': 930, 'incredibly': 931, 'hands': 932, 'bill': 933, 'fantasy': 934, 'return': 935, 'dream': 936, 'apart': 937, 'deep': 938, 'setting': 939, 'admit': 940, 'open': 941, 'appear': 942, 'background': 943, 'christmas': 944, 'leading': 945, 'doctor': 946, 'casting': 947, 'hardly': 948, 'meant': 949, 'ben': 950, 'potential': 951, 'boys': 952, 'powerful': 953, 'business': 954, 'masterpiece': 955, 'fails': 956, 'battle': 957, 'joke': 958, 'create': 959, 'various': 960, 'forward': 961, 'fire': 962, 'inside': 963, 'outside': 964, 'portrayed': 965, 'girlfriend': 966, 'ideas': 967, 'twist': 968, 'william': 969, 'missing': 970, 'nudity': 971, 'reasons': 972, 'villain': 973, 'dumb': 974, 'political': 975, 'deserves': 976, 'match': 977, 'secret': 978, 'expecting': 979, 'air': 980, 'fairly': 981, 'present': 982, 'gay': 983, 'fighting': 984, 'unlike': 985, 'married': 986, 'manages': 987, 'break': 988, 'scott': 989, 'success': 990, 'attempts': 991, 'western': 992, 'spoilers': 993, 'acted': 994, 'remake': 995, 'pay': 996, 'box': 997, 'recently': 998, 'rich': 999, 'front': 1000, 'cute': 1001, 'further': 1002, 'cop': 1003, 'sadly': 1004, 'era': 1005, 'copy': 1006, 'agree': 1007, 'talented': 1008, 'telling': 1009, 'sci-fi': 1010, 'filmmakers': 1011, 'following': 1012, 'public': 1013, 'crew': 1014, 'incredible': 1015, 'missed': 1016, 'wasted': 1017, 'pure': 1018, 'plain': 1019, 'brothers': 1020, 'odd': 1021, 'caught': 1022, 'flat': 1023, 'social': 1024, 'mentioned': 1025, 'pace': 1026, 'ended': 1027, 'decides': 1028, 'considering': 1029, 'waiting': 1030, 'members': 1031, 'mary': 1032, 'large': 1033, 'list': 1034, 'sweet': 1035, 'revenge': 1036, 'popular': 1037, 'uses': 1038, 'hold': 1039, 'slightly': 1040, 'office': 1041, 'compared': 1042, 'neither': 1043, 'wrote': 1044, 'escape': 1045, 'sees': 1046, 'suddenly': 1047, 'e': 1048, 'party': 1049, 'spirit': 1050, 'tension': 1051, 'convincing': 1052, 'created': 1053, 'fear': 1054, 'spent': 1055, 'rate': 1056, 'cause': 1057, 'entirely': 1058, 'island': 1059, 'cartoon': 1060, 'intelligent': 1061, 'credit': 1062, 'clever': 1063, \"we're\": 1064, 'choice': 1065, 'water': 1066, 'kills': 1067, 'bored': 1068, 'familiar': 1069, 'moves': 1070, 'tony': 1071, 'laughing': 1072, 'gun': 1073, 'visual': 1074, 'successful': 1075, 'ultimately': 1076, 'basic': 1077, 'band': 1078, 'trouble': 1079, 'la': 1080, 'cat': 1081, 'zombies': 1082, 'concept': 1083, 'value': 1084, 'biggest': 1085, 'positive': 1086, 'singing': 1087, 'consider': 1088, 'exciting': 1089, 'dancing': 1090, 'company': 1091, 'died': 1092, 'recent': 1093, 'state': 1094, 'language': 1095, 'effective': 1096, 'science': 1097, 'cover': 1098, 'spend': 1099, 'portrayal': 1100, 'violent': 1101, 'cold': 1102, 'appreciate': 1103, 'produced': 1104, 'speak': 1105, 'pointless': 1106, 'former': 1107, 'studio': 1108, 'amusing': 1109, 'adult': 1110, 'common': 1111, 'filled': 1112, 'planet': 1113, 'younger': 1114, 'books': 1115, 'store': 1116, 'walk': 1117, 'follows': 1118, 'focus': 1119, 'solid': 1120, 'bizarre': 1121, 'italian': 1122, 'impressive': 1123, 'animated': 1124, 'impossible': 1125, 'german': 1126, 'amount': 1127, 'tone': 1128, 'century': 1129, 'adventure': 1130, 'conclusion': 1131, 'producers': 1132, \"weren't\": 1133, 'showed': 1134, 'recommended': 1135, 'fit': 1136, 'van': 1137, 'depth': 1138, 'runs': 1139, 'situations': 1140, 'chemistry': 1141, 'prison': 1142, 'project': 1143, 'jim': 1144, 'respect': 1145, 'awesome': 1146, 'control': 1147, 'accent': 1148, 'hair': 1149, 'considered': 1150, 'win': 1151, 'won': 1152, 'smith': 1153, 'force': 1154, 'decide': 1155, 'college': 1156, 'trip': 1157, 'disturbing': 1158, 'somewhere': 1159, 'mad': 1160, 'failed': 1161, 'dad': 1162, 'changed': 1163, 'leaving': 1164, 'questions': 1165, 'longer': 1166, 'steve': 1167, 'audiences': 1168, 'barely': 1169, 'aside': 1170, 'honest': 1171, 'slasher': 1172, 'sick': 1173, 'ghost': 1174, 'shooting': 1175, 'trash': 1176, 'images': 1177, 'thanks': 1178, 'charming': 1179, 'starring': 1180, 'generally': 1181, 'pathetic': 1182, 'literally': 1183, 'west': 1184, 'values': 1185, 'culture': 1186, 'likes': 1187, 'fake': 1188, 'surprisingly': 1189, 'touch': 1190, 'magic': 1191, 'involving': 1192, 'yeah': 1193, 'alive': 1194, 'immediately': 1195, 'stewart': 1196, 'harry': 1197, 'south': 1198, 'frank': 1199, 'garbage': 1200, 'natural': 1201, 'utterly': 1202, 'camp': 1203, 'bought': 1204, 'honestly': 1205, 'adaptation': 1206, 'sam': 1207, 'london': 1208, 'aspect': 1209, 'pictures': 1210, 'ability': 1211, 'detective': 1212, 'nobody': 1213, 'glad': 1214, 'explain': 1215, 'fair': 1216, 'computer': 1217, 'appearance': 1218, 'genius': 1219, 'sitting': 1220, 'cult': 1221, 'attack': 1222, 'master': 1223, 'meaning': 1224, 'personally': 1225, 'stick': 1226, 'normal': 1227, 'u': 1228, 'army': 1229, 'appeal': 1230, 'knowing': 1231, 'tough': 1232, 'remains': 1233, 'humour': 1234, 'military': 1235, 'nowhere': 1236, 'journey': 1237, 'added': 1238, 'charlie': 1239, 'rare': 1240, 'thinks': 1241, 'thank': 1242, 'dreams': 1243, 'c': 1244, 'purpose': 1245, 'touching': 1246, 'unbelievable': 1247, 'comedies': 1248, 'week': 1249, 'catch': 1250, 'taste': 1251, 'terms': 1252, 'chase': 1253, 'beautifully': 1254, 'channel': 1255, 'walking': 1256, 'sexy': 1257, 'batman': 1258, 'terrific': 1259, 'twists': 1260, 'silent': 1261, 'fiction': 1262, 'kelly': 1263, 'wow': 1264, 'standard': 1265, 'equally': 1266, 'naked': 1267, 'mood': 1268, 'subtle': 1269, 'mistake': 1270, 'managed': 1271, 'complex': 1272, 'laughable': 1273, 'pick': 1274, 'themes': 1275, 'road': 1276, 'chris': 1277, 'lovely': 1278, 'narrative': 1279, 'wild': 1280, 'disappointing': 1281, 'likely': 1282, 'brain': 1283, 'costumes': 1284, 'thus': 1285, 'plan': 1286, 'issues': 1287, 'excuse': 1288, 'club': 1289, 'outstanding': 1290, 'soldiers': 1291, 'painful': 1292, 'producer': 1293, 'date': 1294, 'justice': 1295, 'surely': 1296, 'constantly': 1297, 'christopher': 1298, 'government': 1299, 'fully': 1300, 'self': 1301, 'edge': 1302, 'law': 1303, 'boss': 1304, 'presented': 1305, 'victim': 1306, 'cinematic': 1307, 'contains': 1308, 'door': 1309, 'places': 1310, 'central': 1311, 'details': 1312, 'innocent': 1313, 'presence': 1314, 'climax': 1315, 'everybody': 1316, 'slowly': 1317, 'marriage': 1318, 'besides': 1319, 'pass': 1320, 'ride': 1321, 'manner': 1322, 'charles': 1323, 'hoping': 1324, 'animals': 1325, 'historical': 1326, 'charm': 1327, 'stunning': 1328, 'finish': 1329, 'vampire': 1330, 'd': 1331, 'photography': 1332, 'impression': 1333, \"you'd\": 1334, 'henry': 1335, 'loves': 1336, 'gang': 1337, 'spoiler': 1338, 'thrown': 1339, 'mysterious': 1340, 'disappointment': 1341, 'expectations': 1342, 'bottom': 1343, 'hey': 1344, 'shoot': 1345, 'allen': 1346, 'exception': 1347, 'paris': 1348, 'woods': 1349, 'minor': 1350, 'makers': 1351, 'soul': 1352, 'festival': 1353, 'critics': 1354, 'aspects': 1355, 'stands': 1356, 'loud': 1357, 'indian': 1358, 'suppose': 1359, 'scenery': 1360, 'train': 1361, 'bother': 1362, 'cry': 1363, 'church': 1364, 'color': 1365, 'feelings': 1366, 'sent': 1367, 'heavy': 1368, 'support': 1369, 'emotion': 1370, 'bruce': 1371, 'opportunity': 1372, 'award': 1373, 'hotel': 1374, 'brief': 1375, 'element': 1376, 'filming': 1377, 'mainly': 1378, 'ahead': 1379, 'fascinating': 1380, 'forever': 1381, 'blue': 1382, 'rated': 1383, 'acts': 1384, 'pieces': 1385, 'emotions': 1386, 'available': 1387, 'intended': 1388, 'twice': 1389, 'dies': 1390, 'building': 1391, 'names': 1392, 'throw': 1393, 'track': 1394, 'compelling': 1395, 'serial': 1396, 'happening': 1397, 'changes': 1398, 'drawn': 1399, 'hurt': 1400, 'don': 1401, 'puts': 1402, 'green': 1403, 'falling': 1404, 'student': 1405, 'smart': 1406, 'jerry': 1407, 'likable': 1408, 'offer': 1409, 'speaking': 1410, 'tired': 1411, 'suggest': 1412, 'include': 1413, 'bed': 1414, 'pain': 1415, 'difference': 1416, 'victims': 1417, 'confused': 1418, 'adults': 1419, 'lover': 1420, 'followed': 1421, 'impact': 1422, 'f': 1423, 'billy': 1424, 'arthur': 1425, 'approach': 1426, 'giant': 1427, 'motion': 1428, \"hasn't\": 1429, 'appeared': 1430, 'boyfriend': 1431, 'page': 1432, 'developed': 1433, 'bar': 1434, 'image': 1435, 'park': 1436, 'jones': 1437, 'gorgeous': 1438, 'actresses': 1439, 'confusing': 1440, 'trailer': 1441, 'laughed': 1442, 'notice': 1443, 'system': 1444, 'summer': 1445, 'share': 1446, 'lacks': 1447, 'fresh': 1448, 'numbers': 1449, 'alien': 1450, 'fellow': 1451, 'event': 1452, 'grade': 1453, 'supposedly': 1454, 'flaws': 1455, 'martin': 1456, 'murders': 1457, 'moral': 1458, 'zero': 1459, 'content': 1460, 'noir': 1461, 'million': 1462, 'al': 1463, 'opera': 1464, 'mom': 1465, 'gem': 1466, 'ii': 1467, 'relationships': 1468, 'tragedy': 1469, 'answer': 1470, 'mediocre': 1471, 'helps': 1472, 'funniest': 1473, 'drive': 1474, 'merely': 1475, 'lighting': 1476, 'proves': 1477, 'wondering': 1478, 'agent': 1479, 'students': 1480, 'born': 1481, 'random': 1482, 'mix': 1483, 'ray': 1484, 'j': 1485, 'delivers': 1486, 'finding': 1487, 'hospital': 1488, 'creative': 1489, 'christian': 1490, 'putting': 1491, 'key': 1492, 'standards': 1493, 'race': 1494, 'damn': 1495, 'drug': 1496, 'holes': 1497, 'childhood': 1498, 'shock': 1499, 'imagination': 1500, 'impressed': 1501, 'absolute': 1502, 'lived': 1503, 'negative': 1504, 'attractive': 1505, 'land': 1506, 'davis': 1507, 'paid': 1508, 'alan': 1509, 'thoroughly': 1510, 'extreme': 1511, 'rape': 1512, 'flicks': 1513, 'provides': 1514, 'ms': 1515, 'becoming': 1516, 'latter': 1517, 'addition': 1518, 'seemingly': 1519, 'seconds': 1520, 'folks': 1521, 'reminded': 1522, 'flying': 1523, 'fell': 1524, 'brian': 1525, 'ugly': 1526, 'detail': 1527, 'offers': 1528, 'thats': 1529, 'porn': 1530, 'tragic': 1531, 'faces': 1532, 'collection': 1533, 'afraid': 1534, 'intense': 1535, 'lord': 1536, 'affair': 1537, 'six': 1538, 'spot': 1539, 'ship': 1540, 'williams': 1541, 'stuck': 1542, 'lose': 1543, 'seven': 1544, 'industry': 1545, 'forgotten': 1546, 'hidden': 1547, 'queen': 1548, 'soldier': 1549, 'count': 1550, 'nasty': 1551, 'beat': 1552, 'stone': 1553, 'apartment': 1554, 'cliché': 1555, 'adds': 1556, 'jackson': 1557, 'held': 1558, 'castle': 1559, 'rented': 1560, \"shouldn't\": 1561, 'fashion': 1562, 'design': 1563, 'angry': 1564, 'artistic': 1565, 'uncle': 1566, 'therefore': 1567, 'turning': 1568, 'americans': 1569, 'ground': 1570, 'filmmaker': 1571, 'area': 1572, 'pull': 1573, 'information': 1574, 'games': 1575, 'states': 1576, 'anymore': 1577, 'bond': 1578, 'location': 1579, 'super': 1580, 'listen': 1581, 'shocking': 1582, 'describe': 1583, 'personality': 1584, 'jason': 1585, 'lets': 1586, 'step': 1587, 'danny': 1588, 'favourite': 1589, 'scientist': 1590, 'deliver': 1591, 'asks': 1592, 'animal': 1593, 'ready': 1594, 'onto': 1595, 'fox': 1596, 'creature': 1597, 'picked': 1598, 'wooden': 1599, 'inspired': 1600, 'chinese': 1601, 'stephen': 1602, 'intelligence': 1603, 'clothes': 1604, 'news': 1605, 'grace': 1606, 'led': 1607, 'dirty': 1608, 'redeeming': 1609, 'compare': 1610, 'thin': 1611, 'allowed': 1612, 'criminal': 1613, 'member': 1614, 'carry': 1615, 'helped': 1616, 'artist': 1617, 'tears': 1618, 'wonderfully': 1619, 'drugs': 1620, 'struggle': 1621, 'moved': 1622, 'teenage': 1623, 'captain': 1624, 'necessary': 1625, 'desperate': 1626, 'includes': 1627, 'trust': 1628, 'deeply': 1629, 'whatsoever': 1630, 'wars': 1631, 'willing': 1632, 'treat': 1633, 'began': 1634, 'andy': 1635, 'martial': 1636, 'food': 1637, 'direct': 1638, 'g': 1639, 'ed': 1640, 'commentary': 1641, 'quick': 1642, 'nightmare': 1643, 'plane': 1644, 'theatre': 1645, 'heaven': 1646, 'disaster': 1647, 'station': 1648, 'wall': 1649, 'professional': 1650, 'humans': 1651, 'accident': 1652, 'douglas': 1653, 'sleep': 1654, 'phone': 1655, 'cgi': 1656, 'dying': 1657, 'worthy': 1658, 'sky': 1659, 'introduced': 1660, 'superior': 1661, 'energy': 1662, 'johnny': 1663, 'rarely': 1664, 'teacher': 1665, 'sight': 1666, 'warning': 1667, 'comedic': 1668, 'r': 1669, 'independent': 1670, 'anybody': 1671, 'double': 1672, 'eddie': 1673, 'actions': 1674, 'unusual': 1675, 'roll': 1676, 'realized': 1677, 'epic': 1678, 'remarkable': 1679, 'apparent': 1680, 'powers': 1681, 'mouth': 1682, 'allow': 1683, 'mental': 1684, 'wearing': 1685, 'returns': 1686, 'continue': 1687, 'unnecessary': 1688, 'tim': 1689, 'physical': 1690, 'witch': 1691, 'keaton': 1692, 'provide': 1693, 'technical': 1694, 'pleasure': 1695, 'arts': 1696, 'absurd': 1697, 'normally': 1698, 'suicide': 1699, 'desire': 1700, 'anywhere': 1701, 'england': 1702, 'engaging': 1703, 'superman': 1704, 'devil': 1705, 'ford': 1706, 'taylor': 1707, 'fred': 1708, 'limited': 1709, 'skip': 1710, 'surprising': 1711, 'bloody': 1712, 'jr': 1713, 'adam': 1714, 'scared': 1715, 'heroes': 1716, 'hitler': 1717, 'memory': 1718, 'process': 1719, 'wedding': 1720, 'brutal': 1721, 'watchable': 1722, 'suspect': 1723, 'accept': 1724, 'joan': 1725, 'ring': 1726, 'prince': 1727, 'media': 1728, 'machine': 1729, 'russian': 1730, 'intriguing': 1731, 'legend': 1732, 'jeff': 1733, 'somebody': 1734, 'search': 1735, 'suit': 1736, 'wanting': 1737, 'finished': 1738, 'reminds': 1739, 'holds': 1740, 'according': 1741, 'build': 1742, 'cops': 1743, 'torture': 1744, 'academy': 1745, 'vision': 1746, 'pacing': 1747, 'hated': 1748, 'nicely': 1749, 'passion': 1750, 'shakespeare': 1751, 'asked': 1752, 'religious': 1753, 'exist': 1754, 'cage': 1755, 'dick': 1756, 'extra': 1757, 'grand': 1758, 'joy': 1759, 'bits': 1760, 'pilot': 1761, 'faith': 1762, 'growing': 1763, 'clichés': 1764, 'nick': 1765, 'ladies': 1766, 'price': 1767, 'constant': 1768, 'tarzan': 1769, 'blame': 1770, 'smile': 1771, 'originally': 1772, 'explanation': 1773, 'lies': 1774, 'dangerous': 1775, 'kevin': 1776, 'instance': 1777, 'sat': 1778, 'community': 1779, 'jesus': 1780, 'japan': 1781, 'moon': 1782, 'freddy': 1783, 'deserve': 1784, 'river': 1785, 'heroine': 1786, 'met': 1787, 'toward': 1788, 'unknown': 1789, 'capture': 1790, 'gotten': 1791, 'higher': 1792, 'accurate': 1793, 'players': 1794, 'winning': 1795, 'dressed': 1796, 'quiet': 1797, 'whilst': 1798, 'explained': 1799, 'friendship': 1800, 'fail': 1801, 'teen': 1802, \"hadn't\": 1803, 'knowledge': 1804, 'drunk': 1805, 'player': 1806, 'heads': 1807, 'kate': 1808, 'lovers': 1809, 'field': 1810, 'starting': 1811, 'humanity': 1812, 'mike': 1813, 'guns': 1814, 'record': 1815, 'creating': 1816, 'sucks': 1817, 'officer': 1818, 'memories': 1819, 'vhs': 1820, 'judge': 1821, 'jump': 1822, 'villains': 1823, 'cars': 1824, 'pop': 1825, 'horse': 1826, 'finest': 1827, 'fights': 1828, 'issue': 1829, 'featuring': 1830, 'radio': 1831, 'responsible': 1832, 'lacking': 1833, 'morgan': 1834, 'floor': 1835, 'saved': 1836, 'deserved': 1837, 'jimmy': 1838, 'kinda': 1839, 'understanding': 1840, 'lynch': 1841, 'pulled': 1842, 'jean': 1843, 'keeping': 1844, 'delightful': 1845, 'european': 1846, 'rubbish': 1847, 'results': 1848, 'low-budget': 1849, 'treated': 1850, 'hopes': 1851, 'gene': 1852, 'monsters': 1853, 'manage': 1854, 'loving': 1855, 'washington': 1856, 'gary': 1857, 'terribly': 1858, 'months': 1859, 'bland': 1860, 'eat': 1861, 'santa': 1862, 'fate': 1863, 'simon': 1864, 'partner': 1865, 'included': 1866, 'mixed': 1867, 'screaming': 1868, 'broken': 1869, 'sign': 1870, 'mine': 1871, 'numerous': 1872, 'singer': 1873, 'hits': 1874, 'cable': 1875, 'witty': 1876, 'spanish': 1877, 'forces': 1878, 'whenever': 1879, 'empty': 1880, 'author': 1881, 'fat': 1882, 'p': 1883, 'conflict': 1884, 'youth': 1885, 'streets': 1886, 'ball': 1887, 'private': 1888, 'loose': 1889, 'loss': 1890, 'concerned': 1891, 'eric': 1892, 'reviewers': 1893, 'brown': 1894, 'vs': 1895, 'pretentious': 1896, 'skills': 1897, 'werewolf': 1898, 'wind': 1899, 'unfunny': 1900, 'ann': 1901, 'talents': 1902, 'naturally': 1903, 'ordinary': 1904, 'noticed': 1905, 'psychological': 1906, 'discover': 1907, 'opposite': 1908, 'bob': 1909, 'regular': 1910, 'realism': 1911, 'saving': 1912, 'perspective': 1913, 'albert': 1914, 'prove': 1915, 'finale': 1916, 'bigger': 1917, 'morning': 1918, 'dated': 1919, 'driving': 1920, 'sean': 1921, 'anthony': 1922, 'mission': 1923, 'gold': 1924, 'soap': 1925, 'cuts': 1926, 'portray': 1927, 'loses': 1928, 'blonde': 1929, 'locations': 1930, 'length': 1931, 'discovered': 1932, 'bright': 1933, \"we've\": 1934, 'grant': 1935, 'aware': 1936, 'film-making': 1937, 'continues': 1938, 'kong': 1939, 'humorous': 1940, 'below': 1941, 'wood': 1942, 'satire': 1943, 'survive': 1944, 'dealing': 1945, 'international': 1946, 'visit': 1947, 'magnificent': 1948, 'gags': 1949, 'dan': 1950, 'howard': 1951, 'calls': 1952, 'behavior': 1953, 'breaks': 1954, 'owner': 1955, 'candy': 1956, 'shallow': 1957, 'debut': 1958, 'nonsense': 1959, 'murdered': 1960, 'opens': 1961, 'curious': 1962, 'jennifer': 1963, 'visually': 1964, 'trek': 1965, 'golden': 1966, 'connection': 1967, 'context': 1968, 'captured': 1969, 'essentially': 1970, 'sing': 1971, 'shop': 1972, 'miles': 1973, 'deals': 1974, 'bank': 1975, 'advice': 1976, 'm': 1977, 'frankly': 1978, 'occasionally': 1979, 'cameo': 1980, 'revealed': 1981, 'corny': 1982, 'received': 1983, 'blind': 1984, 'harris': 1985, \"they've\": 1986, 'traditional': 1987, 'lesson': 1988, 'genuine': 1989, 'efforts': 1990, 'learned': 1991, 'segment': 1992, 'window': 1993, 'current': 1994, 'gangster': 1995, 'visuals': 1996, 'versions': 1997, 'scream': 1998, 'mrs': 1999, 'luke': 2000, 'develop': 2001, 'o': 2002, 'identity': 2003, 'rob': 2004, 'allows': 2005, 'existence': 2006, 'national': 2007, 'program': 2008, 'sucked': 2009, 'welles': 2010, 'genuinely': 2011, 'comparison': 2012, 'luck': 2013, 'decade': 2014, 'references': 2015, 'unexpected': 2016, 'stock': 2017, 'anna': 2018, 'president': 2019, 'formula': 2020, 'remembered': 2021, 'village': 2022, 'proved': 2023, 'meanwhile': 2024, 'favor': 2025, 'ages': 2026, 'lake': 2027, 'grew': 2028, 'study': 2029, 'brilliantly': 2030, 'sea': 2031, 'vampires': 2032, 'sheer': 2033, 'robin': 2034, 'unable': 2035, 'wise': 2036, 'ice': 2037, 'reach': 2038, 'sake': 2039, 'ultimate': 2040, 'drew': 2041, 'board': 2042, 'steal': 2043, 'sudden': 2044, 'awards': 2045, 'stereotypes': 2046, 'strength': 2047, 'logic': 2048, 'awkward': 2049, 'leader': 2050, 'boat': 2051, 'passed': 2052, 'desert': 2053, 'plots': 2054, 'bet': 2055, 'cross': 2056, 'spectacular': 2057, 'barbara': 2058, 'failure': 2059, 'sinatra': 2060, 'parker': 2061, 'h': 2062, 'halloween': 2063, 'buddy': 2064, 'types': 2065, 'killers': 2066, 'reaction': 2067, 'sheriff': 2068, 'evening': 2069, 'delivered': 2070, 'crappy': 2071, 'bear': 2072, 'gonna': 2073, 'steven': 2074, 'creates': 2075, 'technology': 2076, 'fault': 2077, 'utter': 2078, 'rose': 2079, 'edited': 2080, 'discovers': 2081, 'pair': 2082, 'thomas': 2083, 'laughter': 2084, 'lucky': 2085, 'dreadful': 2086, 'insane': 2087, 'reviewer': 2088, 'flashbacks': 2089, 'families': 2090, 'majority': 2091, 'relief': 2092, 'w': 2093, 'standing': 2094, 'graphic': 2095, 'emotionally': 2096, 'painfully': 2097, 'freedom': 2098, 'ran': 2099, 'woody': 2100, 'caused': 2101, 'parody': 2102, 'gratuitous': 2103, 'decision': 2104, 'site': 2105, 'entertained': 2106, 'religion': 2107, 'meeting': 2108, 'travel': 2109, 'anime': 2110, 'individual': 2111, 'protagonist': 2112, 'attitude': 2113, 'wayne': 2114, 'underrated': 2115, 'nevertheless': 2116, 'speed': 2117, 'victor': 2118, 'foreign': 2119, 'endless': 2120, 'seasons': 2121, 'relate': 2122, 'costs': 2123, 'foot': 2124, 'feet': 2125, 'practically': 2126, 'gordon': 2127, 'pleasant': 2128, 'daniel': 2129, 'test': 2130, 'hill': 2131, 'treatment': 2132, 'france': 2133, 'cinderella': 2134, 'vehicle': 2135, 'twenty': 2136, 'combination': 2137, 'marie': 2138, 'described': 2139, 'classics': 2140, 'v': 2141, 'alex': 2142, 'hunter': 2143, 'victoria': 2144, 'native': 2145, 'gory': 2146, 'jackie': 2147, 'ancient': 2148, 'joseph': 2149, 'irritating': 2150, 'levels': 2151, 'stopped': 2152, 'eating': 2153, 'l': 2154, 'overly': 2155, 'portrays': 2156, 'tape': 2157, 'assume': 2158, 'rules': 2159, 'commercial': 2160, 'hearing': 2161, 'executed': 2162, 'product': 2163, 'haunting': 2164, 'broadway': 2165, 'believes': 2166, 'excited': 2167, 'asking': 2168, 'range': 2169, 'model': 2170, 'theaters': 2171, 'proper': 2172, 'wide': 2173, 'portraying': 2174, 'roy': 2175, 'chosen': 2176, 'wit': 2177, 'chief': 2178, 'sell': 2179, 'uk': 2180, 'angel': 2181, 'rescue': 2182, 'moore': 2183, 'extras': 2184, 'ruined': 2185, 'contrived': 2186, 'generation': 2187, 'capable': 2188, 'recall': 2189, 'embarrassing': 2190, 'largely': 2191, 'matt': 2192, 'unrealistic': 2193, 'marry': 2194, 'clean': 2195, 'produce': 2196, 'round': 2197, 'depressing': 2198, 'fill': 2199, 'nancy': 2200, 'center': 2201, 'handsome': 2202, 'winner': 2203, 'choose': 2204, 'anne': 2205, 'germany': 2206, 'rise': 2207, 'cares': 2208, 'facts': 2209, 'exploitation': 2210, 'matters': 2211, 'suffering': 2212, 'theatrical': 2213, 'learns': 2214, 'ryan': 2215, 'priest': 2216, 'post': 2217, 'sequels': 2218, 'kick': 2219, 'involves': 2220, 'clue': 2221, 'patrick': 2222, 'contrast': 2223, 'sympathetic': 2224, 'grow': 2225, 'fame': 2226, 'appealing': 2227, 'research': 2228, 'strongly': 2229, 'louis': 2230, 'built': 2231, 'asian': 2232, 'excitement': 2233, 'cash': 2234, 'evidence': 2235, 'dry': 2236, 'anderson': 2237, 'disgusting': 2238, 'teenager': 2239, 'correct': 2240, 'lewis': 2241, 'appropriate': 2242, 'claim': 2243, 'chick': 2244, 'vote': 2245, 'pity': 2246, 'heck': 2247, 'talks': 2248, 'losing': 2249, 'destroy': 2250, 'oliver': 2251, 'walter': 2252, 'thoughts': 2253, 'canadian': 2254, 'lousy': 2255, 'circumstances': 2256, 'costume': 2257, 'promise': 2258, 'tedious': 2259, 'crowd': 2260, 'voices': 2261, 'safe': 2262, 'saturday': 2263, 'europe': 2264, 'substance': 2265, 'sports': 2266, 'training': 2267, 'bringing': 2268, 'frame': 2269, 'haunted': 2270, 'bomb': 2271, 'fool': 2272, 'fly': 2273, 'naive': 2274, 'captures': 2275, 'hanging': 2276, 'football': 2277, 'bodies': 2278, 'teenagers': 2279, 'till': 2280, 'amateurish': 2281, 'mask': 2282, 'max': 2283, 'tend': 2284, 'convinced': 2285, 'satisfying': 2286, 'obsessed': 2287, 'welcome': 2288, 'psycho': 2289, 'virtually': 2290, 'hopefully': 2291, 'horribly': 2292, 'asleep': 2293, 'walks': 2294, 'hunt': 2295, 'scare': 2296, 'spoil': 2297, 'cost': 2298, 'powell': 2299, 'danger': 2300, 'relatively': 2301, 'insult': 2302, 'grown': 2303, 'trilogy': 2304, 'steals': 2305, 'africa': 2306, 'murphy': 2307, 'fits': 2308, 'universal': 2309, 'creatures': 2310, 'reporter': 2311, 'unlikely': 2312, 'baseball': 2313, 'che': 2314, 'robot': 2315, 'tiny': 2316, 'dubbed': 2317, 'continuity': 2318, 'seat': 2319, 'influence': 2320, 'market': 2321, 'depicted': 2322, 'flesh': 2323, 'remain': 2324, 'hall': 2325, 'texas': 2326, 'united': 2327, 'kim': 2328, 'initial': 2329, 'fu': 2330, 'unfortunate': 2331, 'offensive': 2332, 'category': 2333, 'send': 2334, 'russell': 2335, 'fare': 2336, 'north': 2337, 'factor': 2338, 'cowboy': 2339, 'soft': 2340, 'believed': 2341, 'ass': 2342, 'lawyer': 2343, 'politics': 2344, 'lower': 2345, 'qualities': 2346, 'provided': 2347, 'target': 2348, 'drag': 2349, 'witness': 2350, 'contemporary': 2351, 'columbo': 2352, 'touches': 2353, 'presents': 2354, 'refreshing': 2355, 'hide': 2356, 'rental': 2357, 'holding': 2358, 'promising': 2359, 'professor': 2360, 'viewed': 2361, 'australian': 2362, 'display': 2363, 'source': 2364, 'amateur': 2365, 'nominated': 2366, 'structure': 2367, 'cartoons': 2368, 'shocked': 2369, 'hitchcock': 2370, 'makeup': 2371, 'forgettable': 2372, 'claims': 2373, 'movement': 2374, 'weekend': 2375, 'sisters': 2376, 'caine': 2377, 'plans': 2378, 'edward': 2379, 'deaths': 2380, 'surreal': 2381, 'closer': 2382, 'clark': 2383, 'handled': 2384, 'ruin': 2385, 'roger': 2386, 'accents': 2387, 'surprises': 2388, 'angles': 2389, 'repeated': 2390, 'serves': 2391, 'chose': 2392, 'adventures': 2393, 'eight': 2394, 'speaks': 2395, 'supernatural': 2396, 'brave': 2397, 'previously': 2398, 'peace': 2399, 'pacino': 2400, 'warm': 2401, 'hat': 2402, 'over-the-top': 2403, 'emma': 2404, 'deadly': 2405, 'granted': 2406, 'degree': 2407, 'latest': 2408, 'suffers': 2409, 'weeks': 2410, 'whoever': 2411, 'cant': 2412, 'rain': 2413, 'skin': 2414, 'accidentally': 2415, 'experiences': 2416, 'highlight': 2417, 'lesbian': 2418, 'covered': 2419, 'service': 2420, 'enemy': 2421, 'dollars': 2422, 'mistakes': 2423, 'pile': 2424, 'routine': 2425, 'paper': 2426, 'treasure': 2427, 'mgm': 2428, 'harsh': 2429, 'uninteresting': 2430, 'alice': 2431, 'realizes': 2432, 'colors': 2433, 'make-up': 2434, 'combined': 2435, 'sympathy': 2436, 'speech': 2437, 'twisted': 2438, 'aliens': 2439, 'friday': 2440, 'veteran': 2441, 'wilson': 2442, 'propaganda': 2443, 'universe': 2444, 'convince': 2445, 'atrocious': 2446, 'dogs': 2447, 'designed': 2448, 'struggling': 2449, 'ted': 2450, 'draw': 2451, 'path': 2452, 'frightening': 2453, \"would've\": 2454, 'mainstream': 2455, 'section': 2456, 'recognize': 2457, 'invisible': 2458, 'lugosi': 2459, 'nude': 2460, 'walked': 2461, 'enter': 2462, 'pitt': 2463, 'print': 2464, 'committed': 2465, 'x': 2466, 'aka': 2467, 'focused': 2468, 'guilty': 2469, 'blah': 2470, 'princess': 2471, 'false': 2472, 'terror': 2473, 'occasional': 2474, 'security': 2475, 'directly': 2476, 'narration': 2477, 'sarah': 2478, 'technically': 2479, 'com': 2480, 'offered': 2481, 'theory': 2482, 'breaking': 2483, 'magical': 2484, 'massive': 2485, 'suspenseful': 2486, 'kinds': 2487, 'variety': 2488, 'experienced': 2489, 'featured': 2490, 'driver': 2491, 'donald': 2492, 'prior': 2493, 'irish': 2494, 'core': 2495, 'subtitles': 2496, 'reputation': 2497, 'explains': 2498, 'fairy': 2499, 'birth': 2500, 'sorts': 2501, 'gritty': 2502, 'bourne': 2503, 'johnson': 2504, 'forest': 2505, 'inner': 2506, 'darkness': 2507, 'stolen': 2508, 'everywhere': 2509, 'performed': 2510, 'abuse': 2511, 'prime': 2512, 'crying': 2513, 'sharp': 2514, 'statement': 2515, 'hong': 2516, 'regret': 2517, 'surface': 2518, 'sunday': 2519, 'gas': 2520, 'reveal': 2521, 'titanic': 2522, 'conversation': 2523, 'chan': 2524, 'remotely': 2525, 'figures': 2526, 'anger': 2527, 'crash': 2528, 'exact': 2529, 'execution': 2530, 'department': 2531, 'dean': 2532, 'required': 2533, 'passing': 2534, 'junk': 2535, 'legendary': 2536, 'beach': 2537, 'multiple': 2538, 'lonely': 2539, 'freeman': 2540, 'rachel': 2541, 'express': 2542, 'hoffman': 2543, 'demons': 2544, 'teens': 2545, 'spy': 2546, 'melodrama': 2547, 'scares': 2548, 'california': 2549, 'mountain': 2550, 'spends': 2551, 'insight': 2552, 'erotic': 2553, 'effectively': 2554, 'grim': 2555, 'julia': 2556, 'grave': 2557, 'downright': 2558, 'crude': 2559, 'revolution': 2560, 'bothered': 2561, 'scale': 2562, 'drop': 2563, 'susan': 2564, 'jon': 2565, 'unconvincing': 2566, 'belief': 2567, 'rule': 2568, 'network': 2569, 'happiness': 2570, 'china': 2571, 'fictional': 2572, 'branagh': 2573, 'figured': 2574, 'nation': 2575, 'paced': 2576, 'favorites': 2577, 'abandoned': 2578, 'afternoon': 2579, 'proud': 2580, 'imagery': 2581, 'beast': 2582, 'alright': 2583, 'minds': 2584, 'blow': 2585, 'worthwhile': 2586, 'dozen': 2587, 'matrix': 2588, 'hired': 2589, 'paint': 2590, 'listening': 2591, 'cutting': 2592, 'forth': 2593, 'blockbuster': 2594, 'delivery': 2595, 'bus': 2596, 'pulls': 2597, 'court': 2598, 'mexico': 2599, 'von': 2600, 'lisa': 2601, 'rings': 2602, 'foster': 2603, 'trapped': 2604, 'dragon': 2605, 'ron': 2606, 'buying': 2607, 'scenario': 2608, 'b-movie': 2609, 'account': 2610, 'urban': 2611, 'burns': 2612, 'amazed': 2613, 'reveals': 2614, 'idiot': 2615, 'larry': 2616, 'quest': 2617, 'jobs': 2618, 'sir': 2619, 'stayed': 2620, 'stays': 2621, 'code': 2622, 'ned': 2623, 'significant': 2624, 'wear': 2625, 'mature': 2626, 'rights': 2627, 'lifetime': 2628, 'examples': 2629, 'paying': 2630, 'studios': 2631, 'jungle': 2632, 'ghosts': 2633, 'angle': 2634, 'mere': 2635, 'status': 2636, 'thankfully': 2637, 'mexican': 2638, 'focuses': 2639, 'changing': 2640, 'necessarily': 2641, \"he'd\": 2642, 'skill': 2643, 'clichéd': 2644, 'vietnam': 2645, 'sidney': 2646, 'suffer': 2647, 'lights': 2648, 'cruel': 2649, 'achieve': 2650, 'spite': 2651, 'interview': 2652, 'african': 2653, 'position': 2654, 'heavily': 2655, 'desperately': 2656, 'san': 2657, 'india': 2658, 'views': 2659, 'destroyed': 2660, 'metal': 2661, 'midnight': 2662, 'murderer': 2663, 'n': 2664, 'summary': 2665, 'dude': 2666, 'artists': 2667, 'initially': 2668, 'fabulous': 2669, 'driven': 2670, 'device': 2671, 'understood': 2672, 'ignore': 2673, 'facial': 2674, 'productions': 2675, 'format': 2676, 'seek': 2677, 'teeth': 2678, 'pregnant': 2679, 'warner': 2680, 'forgot': 2681, 'dennis': 2682, 'closing': 2683, 'hardy': 2684, 'campy': 2685, 'answers': 2686, 'beloved': 2687, 'complicated': 2688, 'placed': 2689, 'bore': 2690, 'deeper': 2691, 'renting': 2692, 'maria': 2693, 'prefer': 2694, 'sounded': 2695, 'settings': 2696, 'ludicrous': 2697, 'raw': 2698, 'table': 2699, 'lincoln': 2700, 'elizabeth': 2701, 'expert': 2702, 'reminiscent': 2703, 'disbelief': 2704, 'encounter': 2705, 'faithful': 2706, 'slapstick': 2707, 'greater': 2708, 'description': 2709, 'racist': 2710, 'brad': 2711, 'inept': 2712, 'league': 2713, 'julie': 2714, 'environment': 2715, 'helping': 2716, 'rolling': 2717, 'sensitive': 2718, 'shadow': 2719, 'flashback': 2720, 'appreciated': 2721, 'task': 2722, 'learning': 2723, 'warned': 2724, 'extraordinary': 2725, 'musicals': 2726, 'storytelling': 2727, 'funnier': 2728, 'claire': 2729, 'mildly': 2730, 'ill': 2731, 'amazingly': 2732, \"ain't\": 2733, 'sleeping': 2734, 'underground': 2735, 'cabin': 2736, 'fourth': 2737, 'decades': 2738, 'depiction': 2739, 'aunt': 2740, 'quirky': 2741, 'halfway': 2742, 'suck': 2743, 'truck': 2744, 'stereotypical': 2745, 'entertain': 2746, 'regarding': 2747, 'cultural': 2748, 'rough': 2749, 'notorious': 2750, 'leslie': 2751, 'sandler': 2752, 'extent': 2753, 'turkey': 2754, 'sun': 2755, 'daughters': 2756, 'sword': 2757, 'kung': 2758, 'criminals': 2759, 'dress': 2760, 'convey': 2761, 'originality': 2762, 'honor': 2763, 'criticism': 2764, 'jessica': 2765, 'arms': 2766, 'calling': 2767, 'expression': 2768, 'drinking': 2769, 'touched': 2770, 'indie': 2771, 'critical': 2772, 'choices': 2773, 'bbc': 2774, 'praise': 2775, 'michelle': 2776, 'buck': 2777, 'purely': 2778, \"could've\": 2779, 'base': 2780, 'experiment': 2781, 'spoof': 2782, 'picks': 2783, 'inspiration': 2784, 'comical': 2785, 'novels': 2786, 'basis': 2787, 'bollywood': 2788, 'demon': 2789, 'notable': 2790, 'prepared': 2791, 'graphics': 2792, 'throwing': 2793, 'breathtaking': 2794, 'cell': 2795, 'flight': 2796, 'miller': 2797, 'serve': 2798, 'succeeds': 2799, 'friendly': 2800, 'introduction': 2801, 'jail': 2802, 'join': 2803, 'throws': 2804, 'properly': 2805, 'lazy': 2806, 'rip': 2807, 'starred': 2808, 'causes': 2809, 'charge': 2810, 'kiss': 2811, 'adding': 2812, 'burt': 2813, 'cooper': 2814, 'raised': 2815, 'stanley': 2816, 'wears': 2817, 'blown': 2818, 'laura': 2819, 'weapons': 2820, 'handle': 2821, 'strangely': 2822, 'brooks': 2823, 'via': 2824, 'nowadays': 2825, 'everyday': 2826, 'lie': 2827, 'rogers': 2828, 'intellectual': 2829, 'protect': 2830, 'caring': 2831, 'lane': 2832, 'blair': 2833, 'challenge': 2834, 'regard': 2835, 'nonetheless': 2836, 'wins': 2837, 'amongst': 2838, 'ratings': 2839, 'raise': 2840, 'nazi': 2841, 'fallen': 2842, 'screening': 2843, 'so-called': 2844, 'escapes': 2845, 'carried': 2846, 'carries': 2847, 'bettie': 2848, 'embarrassed': 2849, 'mirror': 2850, 'tracy': 2851, 'attempting': 2852, 'westerns': 2853, 'gruesome': 2854, 'usa': 2855, 'trick': 2856, 'lion': 2857, 'related': 2858, 'carrying': 2859, 'daily': 2860, 'cases': 2861, 'sinister': 2862, 'alas': 2863, 'breath': 2864, 'arrives': 2865, 'tree': 2866, 'holmes': 2867, 'philip': 2868, 'reed': 2869, 'wave': 2870, 'enjoying': 2871, 'authentic': 2872, 'tales': 2873, 'obnoxious': 2874, 'tradition': 2875, 'pet': 2876, 'busy': 2877, 'navy': 2878, 'replaced': 2879, 'obsession': 2880, 'stanwyck': 2881, 'exists': 2882, 'interpretation': 2883, 'sleazy': 2884, 'southern': 2885, 'mansion': 2886, 'chilling': 2887, 'sitcom': 2888, 'confusion': 2889, 'carpenter': 2890, 'christ': 2891, 'needless': 2892, 'stomach': 2893, 'madness': 2894, 'successfully': 2895, 'ironic': 2896, 'jewish': 2897, 'lucy': 2898, 'reference': 2899, 'remote': 2900, 'timing': 2901, 'presentation': 2902, 'sutherland': 2903, 'locked': 2904, 'attacked': 2905, 'determined': 2906, 'flow': 2907, 'intentions': 2908, 'angels': 2909, 'clips': 2910, 'hood': 2911, 'jake': 2912, 'shut': 2913, 'fortunately': 2914, 'guard': 2915, 'marvelous': 2916, 'punch': 2917, 'ha': 2918, 'rival': 2919, 'essential': 2920, 'horrific': 2921, 'stupidity': 2922, 'legs': 2923, 'titles': 2924, 'ashamed': 2925, 'jumps': 2926, 'term': 2927, 'risk': 2928, 'intensity': 2929, 'interviews': 2930, 'vacation': 2931, 'brando': 2932, 'tight': 2933, 'topic': 2934, 'manager': 2935, 'served': 2936, 'inspector': 2937, 'attacks': 2938, 'established': 2939, 'goofy': 2940, 'remind': 2941, 'comedian': 2942, 'stylish': 2943, 'oddly': 2944, 'albeit': 2945, 'matthau': 2946, 'delight': 2947, 'patient': 2948, 'stops': 2949, 'retarded': 2950, 'sides': 2951, 'seagal': 2952, 'thrilling': 2953, 'cook': 2954, 'contain': 2955, 'jay': 2956, 'suspects': 2957, 'balance': 2958, 'sold': 2959, 'frequently': 2960, 'poignant': 2961, 'dollar': 2962, 'personalities': 2963, 'sullivan': 2964, 'bridge': 2965, 'sum': 2966, 'hence': 2967, 'rochester': 2968, 'struggles': 2969, 'flawed': 2970, 'bo': 2971, 'thief': 2972, 'widmark': 2973, 'revolves': 2974, 'refuses': 2975, 'greatly': 2976, 'screenwriter': 2977, 'drives': 2978, 'internet': 2979, 'shower': 2980, 'grey': 2981, 'expressions': 2982, 'par': 2983, 'baker': 2984, 'bette': 2985, 'upset': 2986, 'overcome': 2987, 'wishes': 2988, 'chair': 2989, 'spoken': 2990, 'elvis': 2991, 'credibility': 2992, 'hole': 2993, 'east': 2994, 'mindless': 2995, 'lessons': 2996, 'innocence': 2997, 'mentally': 2998, 'warn': 2999, 'cynical': 3000, 'franchise': 3001, 'elvira': 3002, 'thousands': 3003, 'riding': 3004, 'lesser': 3005, 'racism': 3006, 'controversial': 3007, 'helen': 3008, 'mouse': 3009, 'dinner': 3010, 'advantage': 3011, 'toy': 3012, 'glass': 3013, 'stranger': 3014, 'corner': 3015, 'credible': 3016, 'mob': 3017, 'italy': 3018, 'fbi': 3019, 'derek': 3020, 'storm': 3021, 'bitter': 3022, 'andrews': 3023, 'bound': 3024, 'text': 3025, 'dentist': 3026, 'wealthy': 3027, 'tense': 3028, 'bugs': 3029, 'dubbing': 3030, 'spielberg': 3031, 'nelson': 3032, 'guessing': 3033, 'streisand': 3034, 'adapted': 3035, 'trial': 3036, 'opened': 3037, 'gross': 3038, 'pride': 3039, 'hundreds': 3040, 'trite': 3041, 'suffered': 3042, ':)': 3043, 'technique': 3044, 'dislike': 3045, 'hills': 3046, 'broke': 3047, 'medical': 3048, 'seeking': 3049, 'reunion': 3050, 'countries': 3051, 'enjoyment': 3052, 'chaplin': 3053, 'kurt': 3054, 'crisis': 3055, 'glimpse': 3056, 'succeed': 3057, 'wasting': 3058, 'performers': 3059, 'sexuality': 3060, 'noble': 3061, 'shines': 3062, 'whereas': 3063, 'ourselves': 3064, 'uncomfortable': 3065, 'britain': 3066, 'infamous': 3067, 'pool': 3068, 'gripping': 3069, 'millions': 3070, 'mass': 3071, 'physically': 3072, 'thrillers': 3073, 'sings': 3074, 'shorts': 3075, 'happily': 3076, 'courage': 3077, 'perform': 3078, 'catholic': 3079, 'separate': 3080, 'hint': 3081, 'atmospheric': 3082, 'drink': 3083, 'contact': 3084, 'ensemble': 3085, 'wwii': 3086, 'oscars': 3087, 'shape': 3088, 'andrew': 3089, 'nine': 3090, 'arnold': 3091, 'curse': 3092, 'karloff': 3093, 'irony': 3094, 'exceptional': 3095, 'cameos': 3096, 'dear': 3097, 'virgin': 3098, 'assistant': 3099, 'cheese': 3100, 'bat': 3101, 'glory': 3102, 'scripts': 3103, 'flynn': 3104, 'weapon': 3105, 'sons': 3106, 'hamlet': 3107, 'horses': 3108, 'suggests': 3109, 'cube': 3110, 'protagonists': 3111, 'ralph': 3112, 'meaningful': 3113, 'idiotic': 3114, 'troubled': 3115, 'burton': 3116, 'steps': 3117, 'quote': 3118, 'vincent': 3119, 'letting': 3120, 'snow': 3121, 'tied': 3122, 'connected': 3123, 'host': 3124, 'consists': 3125, 'perfection': 3126, 'lying': 3127, 'portrait': 3128, 'dancer': 3129, 'sentimental': 3130, 'sin': 3131, 'stretch': 3132, 'bag': 3133, 'hundred': 3134, 'cousin': 3135, 'holiday': 3136, 'develops': 3137, 'plague': 3138, 'unforgettable': 3139, 'noted': 3140, 'gain': 3141, 'lovable': 3142, 'card': 3143, 'worlds': 3144, 'fish': 3145, 'repeat': 3146, 'annoyed': 3147, 'knife': 3148, 'attraction': 3149, 'condition': 3150, 'neighborhood': 3151, 'redemption': 3152, 'accepted': 3153, 'spots': 3154, 'boll': 3155, 'concert': 3156, 'glover': 3157, 'neat': 3158, 'dorothy': 3159, 'torn': 3160, 'roberts': 3161, 'chuck': 3162, 'catherine': 3163, 'saves': 3164, 'colorful': 3165, 'searching': 3166, 'aired': 3167, 'stan': 3168, 'miscast': 3169, 'lucas': 3170, 'hoped': 3171, 'belongs': 3172, 'plastic': 3173, 'hiding': 3174, 'pat': 3175, 'dig': 3176, 'arm': 3177, 'proof': 3178, 'dialogs': 3179, 'denzel': 3180, 'walken': 3181, 'one-liners': 3182, 'object': 3183, 'curtis': 3184, 'boredom': 3185, 'hudson': 3186, 'terrifying': 3187, 'entry': 3188, 'encounters': 3189, 'fortune': 3190, 'equal': 3191, 'eerie': 3192, 'guts': 3193, 'bears': 3194, 'imaginative': 3195, 'month': 3196, 'library': 3197, 'concerns': 3198, 'massacre': 3199, 'aged': 3200, 'worry': 3201, 'chases': 3202, 'hanks': 3203, 'sends': 3204, 'zone': 3205, 'achieved': 3206, 'profound': 3207, 'tune': 3208, 'complaint': 3209, 'tour': 3210, 'stunts': 3211, 'pack': 3212, 'flaw': 3213, 'elsewhere': 3214, 'lloyd': 3215, 'crimes': 3216, 'factory': 3217, 'eva': 3218, 'appearances': 3219, 'endearing': 3220, 'neighbor': 3221, 'segments': 3222, 'expensive': 3223, 'essence': 3224, 'homeless': 3225, 'laid': 3226, 'jeremy': 3227, 'carol': 3228, 'closely': 3229, 'checking': 3230, 'scientists': 3231, 'hooked': 3232, 'canada': 3233, 'solve': 3234, 'hunting': 3235, 'cusack': 3236, 'wing': 3237, 'winter': 3238, 'competent': 3239, 'incoherent': 3240, 'loser': 3241, 'st': 3242, 'rocks': 3243, 'oil': 3244, 'bobby': 3245, 'teach': 3246, 'birthday': 3247, 'dealt': 3248, 'rush': 3249, 'striking': 3250, 'civil': 3251, 'tribute': 3252, 'tricks': 3253, 'battles': 3254, 'rushed': 3255, 'appearing': 3256, 'heston': 3257, 'believing': 3258, 'colour': 3259, 'charisma': 3260, 'countless': 3261, 'dragged': 3262, 'reactions': 3263, 'virus': 3264, 'hang': 3265, 'thousand': 3266, 'videos': 3267, 'dare': 3268, 'attempted': 3269, 'pitch': 3270, 'shoots': 3271, 'chasing': 3272, 'silver': 3273, 'briefly': 3274, 'k': 3275, 'strikes': 3276, 'hip': 3277, 'techniques': 3278, 'magazine': 3279, 'pointed': 3280, 'health': 3281, 'doc': 3282, 'sally': 3283, 'dramas': 3284, 'covers': 3285, 'ritter': 3286, 'unintentionally': 3287, 'horrendous': 3288, 'charismatic': 3289, 'surrounding': 3290, 'surrounded': 3291, 'gag': 3292, 'goal': 3293, 'slight': 3294, 'kane': 3295, 'crafted': 3296, 'barry': 3297, 'eastwood': 3298, 'neck': 3299, 'workers': 3300, 'carter': 3301, 'grows': 3302, 'burning': 3303, 'hearts': 3304, 'tons': 3305, 'dawn': 3306, 'walls': 3307, 'painting': 3308, 'iii': 3309, 'stronger': 3310, 'noise': 3311, 'split': 3312, 'secretary': 3313, 'basement': 3314, 'spike': 3315, 'admittedly': 3316, 'stood': 3317, 'trade': 3318, 'homage': 3319, 'chances': 3320, 'wake': 3321, 'godfather': 3322, 'corrupt': 3323, 'ian': 3324, \"they'd\": 3325, 'cole': 3326, 'requires': 3327, 'nose': 3328, 'easier': 3329, 'fired': 3330, 'spending': 3331, 'pie': 3332, 'intention': 3333, 'weight': 3334, 'cox': 3335, 'attached': 3336, 'specific': 3337, 'ripped': 3338, 'represents': 3339, 'guest': 3340, 'shall': 3341, 'exercise': 3342, 'university': 3343, 'relevant': 3344, 'duke': 3345, 'miike': 3346, 'disagree': 3347, 'stated': 3348, 'guilt': 3349, 'associated': 3350, 'beating': 3351, 'wanna': 3352, 'gothic': 3353, 'sexually': 3354, 'projects': 3355, 'forgive': 3356, 'occurs': 3357, 'stunt': 3358, 'revelation': 3359, 'kicks': 3360, 'messages': 3361, 'carefully': 3362, 'le': 3363, 'tea': 3364, 'stealing': 3365, 'hype': 3366, 'drags': 3367, 'inevitable': 3368, 'identify': 3369, 'medium': 3370, 'notch': 3371, 'acceptable': 3372, 'bakshi': 3373, 'importance': 3374, 'jesse': 3375, 'killings': 3376, 'typically': 3377, 'increasingly': 3378, 'carrey': 3379, 'row': 3380, 'luckily': 3381, 'huh': 3382, 'cardboard': 3383, 'comics': 3384, 'per': 3385, 'gift': 3386, 'dropped': 3387, 'investigation': 3388, 'bush': 3389, 'nights': 3390, 'vague': 3391, 'kubrick': 3392, 'reynolds': 3393, 'lily': 3394, 'savage': 3395, 'thumbs': 3396, 'mild': 3397, 'brand': 3398, 'faced': 3399, 'allowing': 3400, 'brilliance': 3401, 'gotta': 3402, 'fisher': 3403, 'strictly': 3404, 'performing': 3405, 'enters': 3406, 'continued': 3407, 'kudos': 3408, 'toilet': 3409, 'resolution': 3410, 'instantly': 3411, 'bride': 3412, 'kidnapped': 3413, 'destruction': 3414, 'fitting': 3415, 'dawson': 3416, 'contract': 3417, 'breasts': 3418, 'monkey': 3419, 'shining': 3420, 'korean': 3421, 'thrills': 3422, 'afterwards': 3423, 'lab': 3424, 'executive': 3425, 'brosnan': 3426, 'non-existent': 3427, 'fifteen': 3428, 'importantly': 3429, 'pleased': 3430, 'flash': 3431, 'jamie': 3432, 'overlooked': 3433, 'persona': 3434, 'commit': 3435, 'joey': 3436, 'insulting': 3437, 'presumably': 3438, 'competition': 3439, 'press': 3440, 'letter': 3441, 'con': 3442, 'creators': 3443, 'alike': 3444, 'pushed': 3445, 'norman': 3446, 'tremendous': 3447, 'handful': 3448, 'silence': 3449, 'fears': 3450, 'estate': 3451, 'individuals': 3452, 'worthless': 3453, 'must-see': 3454, 'realise': 3455, 'australia': 3456, 'bible': 3457, 'wondered': 3458, 'useless': 3459, 'beings': 3460, 'arrested': 3461, 'aforementioned': 3462, 'mafia': 3463, 'relative': 3464, 'sophisticated': 3465, 'nuclear': 3466, 'regardless': 3467, 'creation': 3468, 'talked': 3469, 'disease': 3470, 'narrator': 3471, 'packed': 3472, 'real-life': 3473, 'doors': 3474, 'astaire': 3475, 'buried': 3476, 'wonders': 3477, 'listed': 3478, 'partly': 3479, 'appalling': 3480, 'strike': 3481, 'returning': 3482, 'cruise': 3483, 'strip': 3484, 'corpse': 3485, 'jerk': 3486, 'menacing': 3487, 'flawless': 3488, 'mitchell': 3489, 'bullets': 3490, 'citizen': 3491, 'heat': 3492, 'planning': 3493, 'pleasantly': 3494, 'highlights': 3495, 'bucks': 3496, 'spell': 3497, 'outrageous': 3498, 'frustrated': 3499, 'union': 3500, 'admire': 3501, 'guide': 3502, 'miserably': 3503, 'grandmother': 3504, 'critic': 3505, 'cagney': 3506, 'nurse': 3507, 'hitting': 3508, 'digital': 3509, 'poverty': 3510, 'ward': 3511, 'evident': 3512, 'abc': 3513, 'satan': 3514, 'ken': 3515, 'perry': 3516, 'shy': 3517, 'fatal': 3518, 'generous': 3519, 'francisco': 3520, 'characterization': 3521, 'consequences': 3522, 'burn': 3523, 'territory': 3524, 'goldberg': 3525, 'diamond': 3526, 'drivel': 3527, 'achievement': 3528, 'horrors': 3529, 'cup': 3530, 'larger': 3531, 'prevent': 3532, 'curiosity': 3533, 'conspiracy': 3534, 'attracted': 3535, 'superbly': 3536, 'twin': 3537, 'boxing': 3538, 'jumping': 3539, 'clues': 3540, 'uninspired': 3541, 'opposed': 3542, 'davies': 3543, 'shortly': 3544, 'motivation': 3545, 'repetitive': 3546, 'accused': 3547, 'picking': 3548, 'slightest': 3549, 'souls': 3550, 'logical': 3551, 'psychiatrist': 3552, 'ambitious': 3553, 'alexander': 3554, 'emily': 3555, 'inspiring': 3556, 'photographed': 3557, 'spooky': 3558, 'blob': 3559, 'ticket': 3560, 'empire': 3561, 'accomplished': 3562, 'falk': 3563, 'splendid': 3564, 'blows': 3565, 'neil': 3566, 'dire': 3567, 'temple': 3568, 'ironically': 3569, 'response': 3570, 'spiritual': 3571, 'los': 3572, 'lou': 3573, 'gandhi': 3574, 'craig': 3575, 'cared': 3576, 'stiff': 3577, 'notes': 3578, 'raped': 3579, 'convoluted': 3580, 'watches': 3581, 'samurai': 3582, 'root': 3583, \"they'll\": 3584, 'felix': 3585, 'morality': 3586, 'installment': 3587, 'indians': 3588, 'directorial': 3589, 'lacked': 3590, 'cameron': 3591, 'suits': 3592, 'timeless': 3593, 'struck': 3594, 'sacrifice': 3595, 'wes': 3596, 'non': 3597, 'manhattan': 3598, 'spring': 3599, 'hire': 3600, 'subsequent': 3601, 'piano': 3602, 'push': 3603, 'revealing': 3604, 'fay': 3605, 'ruth': 3606, 'depression': 3607, 'turner': 3608, 'distant': 3609, 'ignored': 3610, 'ad': 3611, 'em': 3612, 'precious': 3613, 'overdone': 3614, 'documentaries': 3615, 'intrigued': 3616, 'subplot': 3617, 'modesty': 3618, 'hatred': 3619, 'laurel': 3620, 'sole': 3621, 'beer': 3622, 'melodramatic': 3623, 'duo': 3624, 'dracula': 3625, 'farce': 3626, 'remaining': 3627, 'forbidden': 3628, 'spoiled': 3629, 'shark': 3630, 'rip-off': 3631, 'reasonably': 3632, 'pulling': 3633, 'mel': 3634, 'liberal': 3635, 'jealous': 3636, 'ex': 3637, 'wells': 3638, 'drunken': 3639, 'bleak': 3640, 'tunes': 3641, 'elderly': 3642, 'throat': 3643, 'border': 3644, 'marks': 3645, 'paulie': 3646, 'poster': 3647, 'cia': 3648, 'aimed': 3649, 'repeatedly': 3650, 'notably': 3651, 'trap': 3652, 'draws': 3653, 'discussion': 3654, 'failing': 3655, 'editor': 3656, 'explore': 3657, 'obscure': 3658, 'argument': 3659, 'minimal': 3660, 'glenn': 3661, 'verhoeven': 3662, 'tall': 3663, 'craven': 3664, 'outcome': 3665, 'reduced': 3666, 'loosely': 3667, 'photographer': 3668, 'gradually': 3669, 'progress': 3670, 'rap': 3671, 'captivating': 3672, 'timothy': 3673, 'idiots': 3674, 'smooth': 3675, 'explicit': 3676, 'mile': 3677, 'providing': 3678, 'dignity': 3679, 'mid': 3680, 'rid': 3681, 'reasonable': 3682, 'sticks': 3683, 'wicked': 3684, 'wreck': 3685, 'warrior': 3686, 'kenneth': 3687, 'definite': 3688, 'darker': 3689, 'dave': 3690, 'intent': 3691, 'restaurant': 3692, 'misses': 3693, 'screams': 3694, 'sloppy': 3695, 'stiller': 3696, 'futuristic': 3697, 'elaborate': 3698, 'returned': 3699, 'childish': 3700, 'shadows': 3701, 'maker': 3702, 'absence': 3703, 'vicious': 3704, 'horrid': 3705, 'sunshine': 3706, 'areas': 3707, 'mann': 3708, 'danes': 3709, 'meat': 3710, 'annie': 3711, 'brazil': 3712, 'contrary': 3713, 'shoes': 3714, 'worried': 3715, 'jet': 3716, 'unbearable': 3717, 'purple': 3718, 'fx': 3719, 'psychotic': 3720, 'beaten': 3721, 'thirty': 3722, 'unbelievably': 3723, 'blank': 3724, 'cried': 3725, 'warren': 3726, 'romero': 3727, 'hysterical': 3728, 'yesterday': 3729, 'spin': 3730, 'eve': 3731, 'kitchen': 3732, 'mildred': 3733, 'knock': 3734, 'choreography': 3735, 'ireland': 3736, 'exaggerated': 3737, 'pushing': 3738, 'gentle': 3739, 'matthew': 3740, 'triumph': 3741, 'distance': 3742, 'complain': 3743, 'improved': 3744, 'concerning': 3745, 'translation': 3746, 'differences': 3747, 'connect': 3748, 'superhero': 3749, 'eyre': 3750, 'commercials': 3751, 'karen': 3752, 'walker': 3753, 'smoke': 3754, 'stole': 3755, 'anyways': 3756, 'kirk': 3757, 'existed': 3758, 'fancy': 3759, 'displays': 3760, 'smoking': 3761, 'ought': 3762, 'landscape': 3763, 'occurred': 3764, 'tortured': 3765, 'arrive': 3766, 'prom': 3767, 'incident': 3768, 'gray': 3769, 'bathroom': 3770, 'overrated': 3771, 'threat': 3772, 'size': 3773, 'extended': 3774, 'reached': 3775, 'string': 3776, 'nicholson': 3777, 'resembles': 3778, 'currently': 3779, 'symbolism': 3780, 'hamilton': 3781, 'newspaper': 3782, 'farm': 3783, 'scripted': 3784, 'demands': 3785, 'affected': 3786, \"he'll\": 3787, 'blend': 3788, 'donna': 3789, 'movements': 3790, 'drawing': 3791, 'burned': 3792, 'ranks': 3793, \"we'll\": 3794, 'dancers': 3795, 'beatty': 3796, 'panic': 3797, 'hugh': 3798, 'antics': 3799, 'hollow': 3800, 'investigate': 3801, 'pretend': 3802, 'sadness': 3803, 'secrets': 3804, 'website': 3805, 'mate': 3806, 'margaret': 3807, 'holy': 3808, 'broadcast': 3809, 'receive': 3810, 'threw': 3811, 'builds': 3812, 'trio': 3813, 'enjoys': 3814, 'occur': 3815, 'robots': 3816, 'proceedings': 3817, 'recognized': 3818, 'fever': 3819, 'daring': 3820, 'swedish': 3821, 'styles': 3822, 'involvement': 3823, 'wallace': 3824, 'dinosaurs': 3825, 'altogether': 3826, 'imagined': 3827, 'rage': 3828, 'machines': 3829, 'exposed': 3830, 'synopsis': 3831, 'beats': 3832, 'swear': 3833, 'merit': 3834, 'selfish': 3835, 'scientific': 3836, 'fulci': 3837, 'kapoor': 3838, 'broad': 3839, 'discovery': 3840, 'threatening': 3841, 'greek': 3842, 'q': 3843, 'dynamic': 3844, 'giallo': 3845, 'argue': 3846, 'leonard': 3847, 'lawrence': 3848, 'superficial': 3849, 'camera-work': 3850, 'todd': 3851, 'harder': 3852, 'ridiculously': 3853, 'journalist': 3854, 'voight': 3855, 'bell': 3856, 'engaged': 3857, 'clint': 3858, 'heroic': 3859, 'bug': 3860, 'conversations': 3861, 'mountains': 3862, 'thugs': 3863, 'cary': 3864, 'dialogues': 3865, 'cameras': 3866, 'fonda': 3867, 'craft': 3868, 'tame': 3869, 'audio': 3870, 'folk': 3871, 'all-time': 3872, 'apes': 3873, 'bite': 3874, 'adequate': 3875, 'population': 3876, 'unpleasant': 3877, 'sadistic': 3878, 'tommy': 3879, 'et': 3880, 'explosions': 3881, 'innovative': 3882, 'brains': 3883, 'lips': 3884, 'brady': 3885, 'overwhelming': 3886, 'edie': 3887, 'madonna': 3888, 'juvenile': 3889, 'rambo': 3890, 'birds': 3891, 'kidding': 3892, 'blake': 3893, 'titled': 3894, 'prostitute': 3895, 'altman': 3896, 'comfortable': 3897, 'carl': 3898, 'wealth': 3899, 'popcorn': 3900, 'whale': 3901, 'block': 3902, 'offering': 3903, 'intrigue': 3904, 'lemmon': 3905, 'escaped': 3906, 'buddies': 3907, 'clothing': 3908, 'explosion': 3909, 'lol': 3910, 'selling': 3911, 'murderous': 3912, 'aging': 3913, 'genres': 3914, 'orders': 3915, 'cuba': 3916, 'load': 3917, 'upper': 3918, 'nearby': 3919, 'cats': 3920, 'parent': 3921, 'errors': 3922, 'disturbed': 3923, 'atlantis': 3924, 'staff': 3925, 'palma': 3926, 'producing': 3927, 'detailed': 3928, 'ah': 3929, 'groups': 3930, 'mickey': 3931, 'vivid': 3932, 'kennedy': 3933, 'web': 3934, 'rural': 3935, 'liking': 3936, 'implausible': 3937, 'abilities': 3938, 'deliberately': 3939, 'removed': 3940, 'devoted': 3941, 'meaningless': 3942, 'chain': 3943, 'soviet': 3944, 'bin': 3945, 'ingredients': 3946, 'defeat': 3947, 'harvey': 3948, 'damage': 3949, 'causing': 3950, 'possibility': 3951, 'explaining': 3952, 'nightmares': 3953, 'banned': 3954, 'hammer': 3955, 'unwatchable': 3956, 'undoubtedly': 3957, 'nazis': 3958, 'waters': 3959, 'pulp': 3960, 'twelve': 3961, 'headed': 3962, 'holly': 3963, 'composed': 3964, 'relies': 3965, 'olivier': 3966, 'resemblance': 3967, 'philosophy': 3968, 'jeffrey': 3969, 'combat': 3970, 'uneven': 3971, 'unintentional': 3972, 'wolf': 3973, 'sport': 3974, 'grab': 3975, 'eccentric': 3976, 'hbo': 3977, 'butt': 3978, 'distracting': 3979, 'staying': 3980, 'careers': 3981, 'awake': 3982, 'subjects': 3983, 'occasion': 3984, 'signs': 3985, 'focusing': 3986, 'odds': 3987, 'furthermore': 3988, 'versus': 3989, 'officers': 3990, \"it'll\": 3991, 'spare': 3992, 'polanski': 3993, 'masters': 3994, 'passes': 3995, 'react': 3996, 'survival': 3997, 'conventional': 3998, 'celluloid': 3999, 'succeeded': 4000, 'bird': 4001, 'cliff': 4002, 'scheme': 4003, 'catches': 4004, 'mistaken': 4005, 'thrill': 4006, 'mummy': 4007, 'discuss': 4008, 'florida': 4009, 'gadget': 4010, 'decisions': 4011, 'chaos': 4012, 'financial': 4013, 'official': 4014, 'coherent': 4015, 'trees': 4016, 'explored': 4017, 'winters': 4018, 'maggie': 4019, 'spirits': 4020, 'linda': 4021, 'thick': 4022, 'shine': 4023, 'backdrop': 4024, 'funeral': 4025, 'secondly': 4026, 'mysteries': 4027, 'hank': 4028, 'primary': 4029, 'props': 4030, 'pays': 4031, 'travels': 4032, 'flies': 4033, 'colonel': 4034, 'generic': 4035, 'hart': 4036, 'clown': 4037, 'smaller': 4038, 'clumsy': 4039, 'hal': 4040, 'blatant': 4041, 'jazz': 4042, 'afford': 4043, 'garden': 4044, 'notion': 4045, 'exotic': 4046, 'instant': 4047, 'streep': 4048, 'desperation': 4049, 'enemies': 4050, 'toys': 4051, 'settle': 4052, 'brooklyn': 4053, 'ease': 4054, 'fighter': 4055, 'beneath': 4056, 'unfolds': 4057, 'rick': 4058, 'lifestyle': 4059, 'amy': 4060, 'tracks': 4061, 'houses': 4062, 'ginger': 4063, 'ellen': 4064, 'possessed': 4065, 'primarily': 4066, 'leg': 4067, 'principal': 4068, 'hates': 4069, 'devoid': 4070, 'dating': 4071, 'seventies': 4072, 'disc': 4073, 'roman': 4074, 'portion': 4075, 'lyrics': 4076, 'remarkably': 4077, 'motives': 4078, 'destiny': 4079, 'garbo': 4080, 'highest': 4081, 'represent': 4082, 'rex': 4083, 'glorious': 4084, 'enterprise': 4085, 'wannabe': 4086, 'offended': 4087, 'hideous': 4088, 'shirley': 4089, 'montage': 4090, 'frustration': 4091, 'staged': 4092, 'homer': 4093, 'classes': 4094, 'sidekick': 4095, 'wives': 4096, 'widow': 4097, 'companion': 4098, 'countryside': 4099, 'dozens': 4100, 'flop': 4101, 'performer': 4102, 'enormous': 4103, 'dutch': 4104, 'hints': 4105, 'freak': 4106, 'diane': 4107, 'tap': 4108, 'ocean': 4109, 'subtlety': 4110, 'lust': 4111, 'describes': 4112, 'march': 4113, 'stinker': 4114, 'models': 4115, 'ideal': 4116, 'consistently': 4117, 'fond': 4118, 'macy': 4119, 'dances': 4120, 'mixture': 4121, 'bedroom': 4122, 'measure': 4123, 'satisfied': 4124, 'doomed': 4125, 'goodness': 4126, 'disjointed': 4127, 'benefit': 4128, 'directs': 4129, 'reaches': 4130, 'immensely': 4131, 'judging': 4132, 'urge': 4133, 'slave': 4134, 'cheating': 4135, 'li': 4136, 'trailers': 4137, 'impress': 4138, 'akshay': 4139, 'topless': 4140, 'carrie': 4141, 'avoided': 4142, 'april': 4143, 'matches': 4144, 'backgrounds': 4145, 'drops': 4146, 'survivors': 4147, 'amanda': 4148, 'bone': 4149, 'advance': 4150, 'rank': 4151, 'influenced': 4152, 'endings': 4153, 'disappear': 4154, 'senseless': 4155, 'uwe': 4156, 'simplistic': 4157, 'coffee': 4158, 'adorable': 4159, 'diana': 4160, 'relations': 4161, 'winds': 4162, 'blew': 4163, 'tear': 4164, 'contained': 4165, 'specifically': 4166, 'angeles': 4167, 'wont': 4168, 'rebel': 4169, 'slap': 4170, 'outer': 4171, 'stellar': 4172, 'hartley': 4173, 'ego': 4174, 'bold': 4175, 'celebrity': 4176, 'niro': 4177, 'surviving': 4178, 'practice': 4179, 'closet': 4180, 'griffith': 4181, 'soccer': 4182, 'ups': 4183, 'outfit': 4184, 'tender': 4185, 'doll': 4186, 'commented': 4187, 'saga': 4188, 'crush': 4189, 'corruption': 4190, 'bare': 4191, 'revolutionary': 4192, 'ruthless': 4193, 'godzilla': 4194, 'politically': 4195, 'developing': 4196, 'transformation': 4197, 'forms': 4198, 'menace': 4199, 'miserable': 4200, 'claimed': 4201, 'vast': 4202, 'seed': 4203, 'coach': 4204, 'circle': 4205, 'peoples': 4206, 'ties': 4207, 'divorce': 4208, 'surfing': 4209, 'snake': 4210, 'lumet': 4211, 'mario': 4212, 'hardcore': 4213, 'cure': 4214, 'introduces': 4215, 'melting': 4216, 'endure': 4217, 'dedicated': 4218, 'shaw': 4219, 'user': 4220, 'daddy': 4221, 'emphasis': 4222, 'suited': 4223, 'tongue': 4224, 'illogical': 4225, 'authority': 4226, 'disappeared': 4227, 'judy': 4228, 'unsettling': 4229, 'method': 4230, 'sentence': 4231, 'lena': 4232, 'guessed': 4233, 'ears': 4234, 'grinch': 4235, 'involve': 4236, 'link': 4237, 'honesty': 4238, 'cinematographer': 4239, 'loads': 4240, 'depressed': 4241, 'lasted': 4242, 'weakest': 4243, 'jonathan': 4244, 'shelf': 4245, 'arrogant': 4246, 'chorus': 4247, 'similarities': 4248, 'thoughtful': 4249, 'charlotte': 4250, 'berlin': 4251, 'adams': 4252, 'passionate': 4253, 'hung': 4254, 'justify': 4255, 'wrestling': 4256, 'blues': 4257, 'riveting': 4258, 'sellers': 4259, 'recorded': 4260, 'aid': 4261, 'loyal': 4262, 'hurts': 4263, 'terrorist': 4264, 'wore': 4265, 'disappoint': 4266, 'kicked': 4267, 'abysmal': 4268, 'sandra': 4269, 'hit-man': 4270, 'wendy': 4271, 'domino': 4272, 'possibilities': 4273, 'terry': 4274, 'displayed': 4275, 'morris': 4276, 'interaction': 4277, 'monkeys': 4278, 'moody': 4279, 'traveling': 4280, 'quotes': 4281, 'isolated': 4282, 'stinks': 4283, 'represented': 4284, 'realizing': 4285, 'faster': 4286, 'cards': 4287, 'solo': 4288, 'yellow': 4289, 'improve': 4290, 'oz': 4291, 'planned': 4292, 'understandable': 4293, 'treats': 4294, 'preview': 4295, 'bonus': 4296, 'buff': 4297, 'considerable': 4298, 'grasp': 4299, 'command': 4300, 'solely': 4301, 'nomination': 4302, 'trashy': 4303, 'christy': 4304, 'campbell': 4305, 'stereotype': 4306, 'cake': 4307, 'rooms': 4308, 'dalton': 4309, 'severe': 4310, 'formulaic': 4311, 'macarthur': 4312, 'stress': 4313, 'faults': 4314, 'chased': 4315, 'tad': 4316, 'tonight': 4317, 'roth': 4318, 'snl': 4319, 'splatter': 4320, 'elephant': 4321, 'dinosaur': 4322, 'mayor': 4323, 'bullet': 4324, 'nostalgic': 4325, 'embarrassment': 4326, 'alert': 4327, 'facing': 4328, 'comparing': 4329, 'unhappy': 4330, 'report': 4331, 'namely': 4332, 'purchase': 4333, 'khan': 4334, 'orson': 4335, 'lately': 4336, 'safety': 4337, 'scope': 4338, 'horrifying': 4339, 'eighties': 4340, 'leo': 4341, 'dolls': 4342, 'ignorant': 4343, 'promised': 4344, 'consistent': 4345, 'helicopter': 4346, 'inventive': 4347, 'museum': 4348, 'hopper': 4349, 'downhill': 4350, 'kyle': 4351, 'photos': 4352, 'crack': 4353, 'swimming': 4354, 'letters': 4355, 'waited': 4356, 'taught': 4357, 'hook': 4358, 'one-dimensional': 4359, 'nervous': 4360, 'ruby': 4361, 'simpson': 4362, 'handed': 4363, 'scarecrow': 4364, 'edgar': 4365, 'agrees': 4366, 'ollie': 4367, 'concern': 4368, 'montana': 4369, 'on-screen': 4370, 'shelley': 4371, 'kingdom': 4372, 'myers': 4373, 'sits': 4374, 'motivations': 4375, 'philosophical': 4376, 'similarly': 4377, 'punk': 4378, 'mall': 4379, 'poetic': 4380, 'switch': 4381, 'wished': 4382, 'inane': 4383, 'buffs': 4384, 'rocket': 4385, 'fix': 4386, 'corporate': 4387, 'ballet': 4388, 'blond': 4389, 'behave': 4390, 'porno': 4391, 'education': 4392, 'ya': 4393, 'dickens': 4394, 'amitabh': 4395, 'rotten': 4396, 'defend': 4397, 'virginia': 4398, 'useful': 4399, 'grandfather': 4400, 'jenny': 4401, 'painted': 4402, 'sh': 4403, 'fooled': 4404, 'stevens': 4405, 'bergman': 4406, 'connery': 4407, 'blunt': 4408, 'duty': 4409, 'relation': 4410, 'delivering': 4411, 'vegas': 4412, 'chess': 4413, 'boot': 4414, 'gangsters': 4415, 'stooges': 4416, 'artificial': 4417, 'transfer': 4418, 'franco': 4419, 'buildings': 4420, 'paltrow': 4421, 'closest': 4422, 'miracle': 4423, 'suitable': 4424, 'carradine': 4425, 'inferior': 4426, 'combine': 4427, 'timon': 4428, 'convincingly': 4429, 'humble': 4430, 'shirt': 4431, 'affect': 4432, 'agents': 4433, 'engage': 4434, 'mars': 4435, 'cg': 4436, 'cd': 4437, 'global': 4438, 'arrived': 4439, 'valuable': 4440, 'reads': 4441, 'pretending': 4442, 'tiresome': 4443, 'dixon': 4444, 'couples': 4445, 'conflicts': 4446, 'vengeance': 4447, 'robinson': 4448, 'pearl': 4449, 'sappy': 4450, 'racial': 4451, 'close-ups': 4452, 'thru': 4453, 'airplane': 4454, 'dub': 4455, 'emperor': 4456, 'plant': 4457, 'maintain': 4458, 'scrooge': 4459, 'bela': 4460, 'staring': 4461, 'provoking': 4462, 'finger': 4463, 'sirk': 4464, 'cheated': 4465, 'wizard': 4466, 'pokemon': 4467, 'waves': 4468, 'steel': 4469, 'scottish': 4470, 'elegant': 4471, 'willis': 4472, 'pig': 4473, 'babe': 4474, 'reaching': 4475, 'nicholas': 4476, 'witches': 4477, 'access': 4478, 'communist': 4479, 'bath': 4480, 'blade': 4481, 'crocodile': 4482, 'civilization': 4483, 'secretly': 4484, 'ethan': 4485, 'yelling': 4486, 'airport': 4487, 'bands': 4488, 'gerard': 4489, 'september': 4490, 'beliefs': 4491, 'species': 4492, 'spock': 4493, 'destroying': 4494, 'lundgren': 4495, 'gundam': 4496, 'reflect': 4497, 'greedy': 4498, 'francis': 4499, 'advertising': 4500, 'limits': 4501, 'aids': 4502, 'abusive': 4503, 'questionable': 4504, 'im': 4505, 'made-for-tv': 4506, 'robbery': 4507, 'germans': 4508, 'wound': 4509, 'nostalgia': 4510, 'progresses': 4511, 'stallone': 4512, 'shake': 4513, 'stilted': 4514, 'richards': 4515, 'remarks': 4516, 'centers': 4517, 'earned': 4518, 'potentially': 4519, 'chicago': 4520, 'guarantee': 4521, 'rat': 4522, 'scores': 4523, 'catchy': 4524, 'wet': 4525, 'misery': 4526, 'designs': 4527, 'armed': 4528, 'desired': 4529, 'alcoholic': 4530, 'dimensional': 4531, 'plight': 4532, 'exploration': 4533, 'latin': 4534, 'literature': 4535, 'firstly': 4536, 'richardson': 4537, 'lay': 4538, 'simplicity': 4539, 'cleverly': 4540, 'wandering': 4541, 'marty': 4542, 'understated': 4543, 'sub': 4544, 'vulnerable': 4545, 'arguably': 4546, 'intentionally': 4547, 'invasion': 4548, 'complexity': 4549, 'careful': 4550, 'rendition': 4551, 'alongside': 4552, 'illegal': 4553, 'brutally': 4554, 'cave': 4555, 'amounts': 4556, 'showcase': 4557, 'chest': 4558, 'construction': 4559, 'depicts': 4560, 'purchased': 4561, 'bottle': 4562, 'lengthy': 4563, 'survived': 4564, 'iran': 4565, 'voice-over': 4566, 'advise': 4567, 'homosexual': 4568, 'austen': 4569, 'iron': 4570, 'attitudes': 4571, 'trained': 4572, 'terrorists': 4573, 'confidence': 4574, 'recognition': 4575, 'transition': 4576, 'restored': 4577, 'prisoner': 4578, 'paranoia': 4579, 'rising': 4580, 'sue': 4581, 'spoke': 4582, 'damon': 4583, 'persons': 4584, 'spread': 4585, 'conservative': 4586, 'witnesses': 4587, 'visits': 4588, 'junior': 4589, 'demented': 4590, 'experiments': 4591, 'biko': 4592, 'mini': 4593, 'farrell': 4594, 'drake': 4595, 'bull': 4596, 'wrapped': 4597, 'likewise': 4598, 'awe': 4599, 'inappropriate': 4600, 'floating': 4601, 'bacall': 4602, 'gabriel': 4603, 'tie': 4604, 'attorney': 4605, 'illness': 4606, 'resources': 4607, 'grayson': 4608, 'worker': 4609, 'relatives': 4610, 'ashley': 4611, 'raymond': 4612, 'justin': 4613, 'neighbors': 4614, 'scooby': 4615, 'opinions': 4616, 'phantom': 4617, 'hello': 4618, 'troops': 4619, 'mankind': 4620, 'molly': 4621, 'patients': 4622, 'incomprehensible': 4623, 'peak': 4624, 'frankenstein': 4625, 'joined': 4626, 'raising': 4627, 'jumped': 4628, 'advanced': 4629, 'slick': 4630, 'pants': 4631, 'viewings': 4632, 'edition': 4633, 'witnessed': 4634, 'poetry': 4635, 'visible': 4636, 'tomatoes': 4637, 'hunters': 4638, 'foul': 4639, 'hyde': 4640, 'fascinated': 4641, 'dreary': 4642, 'choreographed': 4643, 'suspicious': 4644, 'creator': 4645, 'intimate': 4646, 'accompanied': 4647, 'online': 4648, 'assault': 4649, 'copies': 4650, 'pit': 4651, 'june': 4652, 'lively': 4653, 'antwone': 4654, 'taxi': 4655, 'classical': 4656, 'agreed': 4657, 'appreciation': 4658, 'capturing': 4659, 'ensues': 4660, 'losers': 4661, 'carell': 4662, 'bang': 4663, 'responsibility': 4664, 'pops': 4665, 'proceeds': 4666, 'resort': 4667, 'cities': 4668, 'lowest': 4669, 'challenging': 4670, 'unit': 4671, 'button': 4672, 'signed': 4673, 'keith': 4674, 'subplots': 4675, 'barrymore': 4676, 'alison': 4677, 'kicking': 4678, 'parallel': 4679, 'owen': 4680, 'pot': 4681, 'lit': 4682, 'frequent': 4683, 'descent': 4684, 'manipulative': 4685, 'belushi': 4686, 'earl': 4687, 'royal': 4688, 'butler': 4689, 'excessive': 4690, 'rooney': 4691, 'betty': 4692, 'satisfy': 4693, 'eaten': 4694, 'solution': 4695, 'del': 4696, 'alternate': 4697, 'ape': 4698, 'othello': 4699, 'arrival': 4700, 'overacting': 4701, 'eager': 4702, 'kay': 4703, 'randy': 4704, 'christians': 4705, 'capital': 4706, 'compelled': 4707, 'stale': 4708, 'pal': 4709, 'lone': 4710, 'wisdom': 4711, 'incompetent': 4712, 'parties': 4713, 'mistress': 4714, 'basketball': 4715, 'writes': 4716, 'hilariously': 4717, 'crucial': 4718, 'graham': 4719, 'domestic': 4720, 'constructed': 4721, 'nyc': 4722, 'smiling': 4723, 'prize': 4724, 'sincere': 4725, 'der': 4726, 'warmth': 4727, 'property': 4728, 'tends': 4729, 'mundane': 4730, 'creep': 4731, 'twilight': 4732, 'waitress': 4733, 'equivalent': 4734, 'masterful': 4735, 'well-known': 4736, 'louise': 4737, 'balls': 4738, 'reid': 4739, 'exceptionally': 4740, 'plausible': 4741, 'overlook': 4742, 'rukh': 4743, 'seeks': 4744, 'enthusiasm': 4745, 'mentions': 4746, 'slaughter': 4747, 'rangers': 4748, 'despair': 4749, 'irrelevant': 4750, 'pursuit': 4751, 'frankie': 4752, 'chicks': 4753, 'vaguely': 4754, 'channels': 4755, 'spain': 4756, 'blowing': 4757, 'randomly': 4758, 'loaded': 4759, 'cliche': 4760, 'wishing': 4761, 'tiger': 4762, 'novak': 4763, 'calm': 4764, 'widely': 4765, 'dolph': 4766, 'operation': 4767, 'methods': 4768, 'zizek': 4769, 'julian': 4770, 'pamela': 4771, 'demand': 4772, 'landscapes': 4773, 'dust': 4774, 'chooses': 4775, 'watson': 4776, 'abraham': 4777, 'belong': 4778, 'priceless': 4779, 'legal': 4780, 'whats': 4781, 'prisoners': 4782, 'angela': 4783, 'improvement': 4784, 'poem': 4785, 'instinct': 4786, 'greed': 4787, 'opportunities': 4788, 'analysis': 4789, 'tag': 4790, 'nuts': 4791, 'travesty': 4792, 'borrowed': 4793, 'elm': 4794, 'icon': 4795, 'israel': 4796, 'brenda': 4797, 'polished': 4798, 'household': 4799, 'masterpieces': 4800, 'gentleman': 4801, 'prequel': 4802, 'alfred': 4803, 'creativity': 4804, 'meryl': 4805, 'imitation': 4806, 'awfully': 4807, 'bumbling': 4808, 'mechanical': 4809, 'muslim': 4810, 'fido': 4811, 'recording': 4812, 'feed': 4813, 'wounded': 4814, 'tooth': 4815, 'thompson': 4816, 'walsh': 4817, 'simmons': 4818, 'mentioning': 4819, 'cassidy': 4820, 'rabbit': 4821, 'suffice': 4822, 'troubles': 4823, 'doom': 4824, 'willie': 4825, 'assassin': 4826, 'introduce': 4827, 'equipment': 4828, 'empathy': 4829, 'square': 4830, 'whoopi': 4831, 'palace': 4832, 'carla': 4833, 'psychic': 4834, 'heights': 4835, 'marion': 4836, 'tribe': 4837, 'survivor': 4838, 'conceived': 4839, 'omen': 4840, 'pan': 4841, 'resist': 4842, 'performs': 4843, 'promises': 4844, 'iraq': 4845, 'punishment': 4846, 'chapter': 4847, 'scotland': 4848, 'distinct': 4849, 'peters': 4850, 'assigned': 4851, 'showdown': 4852, 'acid': 4853, 'activities': 4854, 'wacky': 4855, 'pink': 4856, 'bud': 4857, 'ustinov': 4858, 'rats': 4859, 'rocky': 4860, 'damme': 4861, 'map': 4862, 'trail': 4863, 'fuller': 4864, 'nephew': 4865, 'receives': 4866, 'bay': 4867, 'maid': 4868, 'voiced': 4869, 'climactic': 4870, 'unaware': 4871, 'cannibal': 4872, 'josh': 4873, 'jaws': 4874, 'greg': 4875, 'historically': 4876, 'messed': 4877, 'masses': 4878, 'soderbergh': 4879, 'cringe': 4880, 'attend': 4881, 'spacey': 4882, 'petty': 4883, 'sink': 4884, 'resident': 4885, 'ramones': 4886, 'unreal': 4887, 'ruins': 4888, 'album': 4889, 'expressed': 4890, 'monk': 4891, 'phony': 4892, 'unoriginal': 4893, 'fury': 4894, 'quit': 4895, 'suspend': 4896, 'purposes': 4897, 'marketing': 4898, 'mclaglen': 4899, 'darren': 4900, 'sabrina': 4901, 'championship': 4902, 'dana': 4903, 'closed': 4904, 'doubts': 4905, 'interests': 4906, 'integrity': 4907, 'minimum': 4908, 'pierce': 4909, 'swim': 4910, 'resulting': 4911, 'popularity': 4912, 'depicting': 4913, 'eugene': 4914, 'miyazaki': 4915, 'sid': 4916, 'kissing': 4917, 'shed': 4918, 'edgy': 4919, 'greatness': 4920, 'warriors': 4921, 'dimension': 4922, 'roots': 4923, 'testament': 4924, 'assumed': 4925, 'composer': 4926, 'nicole': 4927, 'firm': 4928, 'inevitably': 4929, 'contest': 4930, 'businessman': 4931, 'correctly': 4932, 'sissy': 4933, 'wholly': 4934, 'underlying': 4935, 'dud': 4936, 'unpredictable': 4937, 'austin': 4938, 'mighty': 4939, 'nathan': 4940, 'teaching': 4941, 'gifted': 4942, 'sixties': 4943, 'uplifting': 4944, 'wang': 4945, 'exposure': 4946, 'furious': 4947, 'nail': 4948, 'trademark': 4949, 'kurosawa': 4950, 'accuracy': 4951, 'sneak': 4952, 'laughably': 4953, 'compassion': 4954, 'film-makers': 4955, 'dreck': 4956, 'expedition': 4957, 'boom': 4958, 'orleans': 4959, 'definition': 4960, 'educational': 4961, 'suggested': 4962, 'antonioni': 4963, 'patience': 4964, 'catching': 4965, 'defense': 4966, 'twins': 4967, 'les': 4968, 'stunned': 4969, 'dee': 4970, 'supported': 4971, 'merits': 4972, 'moronic': 4973, 'directions': 4974, 'specially': 4975, 'package': 4976, 'distribution': 4977, 'harm': 4978, 'hopeless': 4979, 'deniro': 4980, 'valley': 4981, 'marshall': 4982, 'resemble': 4983, 'seldom': 4984, 'quinn': 4985, 'biography': 4986, 'yard': 4987, 'brutality': 4988, 'adaptations': 4989, 'kumar': 4990, 'immediate': 4991, 'strangers': 4992, 'reflection': 4993, 'fashioned': 4994, 'maniac': 4995, 'malone': 4996, 'victory': 4997, 'confess': 4998, 'sounding': 4999, 'pete': 5000, 'preston': 5001, 'significance': 5002}\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have a vocabulary, we can process the unsupervised examples we loaded earlier into actual training data our model can read.\n",
    "\n",
    "First, we'll tokenize the text normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block may take a while (<5 minutes) to run, but you only have to run it once, so make sure you don't modify the tokenized_examples list after it's completed.\n",
    "While you're writing your code, consider limiting unsup_examples to the first 5 examples as a smoke test before you run the loop over all examples\n",
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_examples = []\n",
    "sos_id = vocabulary['<s>'] #start of sequence\n",
    "eos_id = vocabulary['</s>'] #end of sequence\n",
    "unk_id = vocabulary['<unk>']\n",
    "\n",
    "for example in unsup_examples:\n",
    "    example_tokens = [token.lower() for token in word_tokenize(example)]\n",
    "\n",
    "    token_ids = [sos_id]\n",
    "    for token in example_tokens:\n",
    "        '''\n",
    "            Your code here.\n",
    "\n",
    "            The above loop iterates over the tokens in a single example. If a token is in our vocabulary, then add it to token_ids. If not, add the unknown token.\n",
    "\n",
    "            10 pts.\n",
    "            \n",
    "\n",
    "        '''\n",
    "\n",
    "\n",
    "    token_ids.append(eos_id)\n",
    "    tokenized_examples.append(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4g/9n6gzty13b758rhwfbvxwx980000gn/T/ipykernel_94926/2040143394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m191\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mtokenized_examples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m940\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m86\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2091\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m107\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m618\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m158\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m134\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m43\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m22\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m71\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2587\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m653\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2361\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m440\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2260\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m108\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m51\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m522\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2648\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1517\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3053\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m468\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m388\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m854\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1103\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m345\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2738\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m294\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m39\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m471\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m154\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m115\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1731\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m695\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m387\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m473\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m410\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m77\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m44\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m291\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m228\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1796\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m84\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m943\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m367\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m291\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2587\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m334\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1320\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1267\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m444\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m323\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m66\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m173\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m377\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m598\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m387\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1951\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert len(tokenized_examples[0]) == 191\n",
    "assert tokenized_examples[0] == [0, 11, 940, 2, 3, 86, 2091, 6, 107, 618, 158, 134, 2, 25, 43, 22, 16, 71, 2, 6, 3, 2587, 42, 38, 2, 653, 2, 2, 11, 27, 2361, 2, 29, 11, 440, 2, 3, 2260, 2, 2, 4, 108, 70, 55, 51, 2, 3, 229, 4300, 4, 522, 2648, 2, 13, 1517, 3053, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 38, 11, 14, 2, 44, 12, 29, 2, 4, 468, 8, 388, 854, 7, 1103, 2, 2, 2, 345, 2, 2738, 294, 2, 11, 120, 39, 3, 471, 154, 2, 19, 201, 115, 6, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 29, 1731, 695, 2, 387, 21, 2, 3, 473, 25, 410, 7, 77, 2, 2, 2, 4, 16, 44, 291, 228, 2, 1300, 1796, 2, 84, 50, 2, 2, 10, 3, 943, 2, 367, 291, 2587, 334, 1320, 33, 2, 31, 1267, 2, 444, 4, 323, 2, 2, 49, 2, 2, 4, 16, 90, 2, 66, 2, 2, 173, 42, 377, 2, 2, 25, 598, 2, 14, 12, 34, 387, 2, 6, 2, 2, 6, 1951, 2, 49, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our bigram data. We'll make use of the torch Dataset class. We only need to implement the `__getitem__` and `__len__` methods to make this work with other existing torch tools.\n",
    "\n",
    "For this dataset, for each example, iterate over its bigrams. If either one of the tokens is an unknown token, then do not save the bigram. Since we're using a small vocabulary, we'll have a lot of unknowns, and we don't want our model to always predict this token as the most likely next token.\n",
    "\n",
    "Note that with a normal sized vocabulary, training set, and model, you wouldn't necessarily want to do this -- unknowns would hopefully be relatively rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BigramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_data):\n",
    "\n",
    "        self.examples = []\n",
    "        for example in tokenized_data:              #Iterate over our dataset\n",
    "            for i in range(0,len(example) - 1):     #Iterate over the tokens of the example\n",
    "                '''\n",
    "                    Your code here.\n",
    "\n",
    "                    Bigrams should be a tuple of integers: (example[i], example[i+1])\n",
    "                    For each bigram, if either of example[i] or example[i+1] are unknown then do not add the bigram to our examples.\n",
    "\n",
    "                    10 pts.\n",
    "                '''\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the bigram model. This is similar to the one in class: the input is a single token, and the model outputs a probability over the whole vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_layer_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the model. This training loop is similar to the one shown in lecture, with a couple of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "\n",
    "#Initialize our model -- keep it small with 1 hidden layer, and embedding sizes of 50\n",
    "bigram_model = BigramLM(len(vocabulary), 50, 50, 1)\n",
    "\n",
    "#Initialize our dataset using a subset of examples\n",
    "bigram_dataset = BigramDataset(tokenized_examples[:5000])\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters())            #AdamW is a popularly used optimizer\n",
    "# optimizer = torch.optim.SGD(bigram_model.parameters(), lr=0.5)    #Either of these optimizers could be used\n",
    "\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "print_frequency = 1000\n",
    "\n",
    "#We'll create an instance of a torch dataloader to collate our data. This class handles batching and shuffling (should be done each epoch)\n",
    "train_dataloader = torch.utils.data.DataLoader(bigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "    bigram_model.train()\n",
    "    avg_loss = 0\n",
    "\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "\n",
    "        x, y = data\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_output = bigram_model(x)\n",
    "        model_output_probabilities = softmax(model_output)\n",
    "\n",
    "        loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        if step % print_frequency == 1:\n",
    "            print('epoch: {} batch: {} loss: {}'.format(\n",
    "                i,\n",
    "                step,\n",
    "                avg_loss / print_frequency\n",
    "            ))\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the loop above to train the model for at least 1 epoch.\n",
    "\n",
    "1. Modify the loop to keep track of the average loss before it's reset. Then, plot the losses using matplotlib below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here.\n",
    "\n",
    "The average loss is reset after print_frequency iterations. Before it's set to 0, store it in a list that will persist throughout training.\n",
    "\n",
    "10 pts.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll modify our model and dataset to create a trigram language model. Here, the input will be two words rather than 1. The output will remain the same.\n",
    "\n",
    "Hint: since we have two inputs, we'll want to combine them in some way after we get their embeddings. An easy way to do this would be to concatenate the two embeddings together, creating a new vector of size 2*embedding_dim. This will be the input size of the first hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TrigramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_data):\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TrigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "        self.hidden_layer_1 = nn.Linear(embedding_dim, hidden_dim)  #Embedding_dim will have to be modified\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        # Hint: we'll need to get an embedding for our second input somehow\n",
    "        # self.embedding_1 = self.embedding(input_1)\n",
    "        # self.embedding_2 =\n",
    "\n",
    "        # Hint: This might be one way to combine our embeddings\n",
    "        # self.embedding = torch.cat()\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_trigram(trigram_model, trigram_dataset):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(trigram_model.parameters())\n",
    "    # optimizer = torch.optim.SGD(trigram_model.parameters(), lr=0.5)\n",
    "\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    print_frequency = 1000\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(trigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        trigram_model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = trigram_model(x)\n",
    "            model_output_probabilities = softmax(model_output)\n",
    "\n",
    "            loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Complete the following code block by initializing the model/dataset and training for at least one epoch\n",
    "Hint: the models and dataset should be _extremely_ similar to the bigram model and dataset\n",
    "\n",
    "20 pts.\n",
    "\"\"\"\n",
    "\n",
    "trigram_model = TrigramLM()\n",
    "trigram_dataset = TrigramDataset()\n",
    "\n",
    "\n",
    "train_trigram(trigram_model, trigram_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this section, complete the Trigram model and dataset, and train the model for at least 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sentiment Analysis\n",
    "\n",
    "In this section we'll compare how a neural model similar to the one above performs on sentiment analysis. Then, we'll replace the embeddings with pretrained ones to see if that increases our performance. To make our life easier, we'll use the glove vocabulary for both models.\n",
    "\n",
    "You can download the embeddings from here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The glove vectors are distributed as a text file, with the word in the first column, and the embeddings in the remaining columns. We'll read in the embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = numpy.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "embedding_matrix = numpy.zeros((len(embeddings_dict)+1, 50)) #add 1 for padding\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys()):\n",
    "\n",
    "    word2id[word] = i                                #Map each word to an index\n",
    "    embedding_matrix[i] = embeddings_dict[word]      #That index holds the Glove embedding in the embedding matrix\n",
    "\n",
    "# Our joint vocabulary for both models / sanity check to see if we've loaded it correctly:\n",
    "print(word2id['the'])\n",
    "print(embedding_matrix[word2id['the']])\n",
    "\n",
    "word2id['<pad>'] = embedding_matrix.shape[0] - 1\n",
    "print(embedding_matrix[word2id['<pad>']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create another dataset for our (now labeled) movie reviews. Do not change the max_length values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a classification dataset for the movie reviews\n",
    "\n",
    "\n",
    "class MovieReviewDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, directory=None, split=None, word2id=None, finalized_data=None, data_limit=250, max_length=256):\n",
    "        \"\"\"\n",
    "        :param directory: The location of aclImdb\n",
    "        :param split: Train or test\n",
    "        :param word2id: The generated glove word2id dictionary\n",
    "        :param finalized_data: We'll use this to initialize a validation set without reloading the data.\n",
    "        :param data_limit: Limiter on the number of examples we load\n",
    "        :param max_length: Maximum length of the sequence\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_limit = data_limit\n",
    "        self.max_length = max_length\n",
    "        self.word2id = word2id\n",
    "\n",
    "        if finalized_data:\n",
    "            self.data = finalized_data\n",
    "\n",
    "        else:\n",
    "\n",
    "            pos_dir = directory + '{}/pos/'.format(split)\n",
    "            neg_dir = directory + '{}/neg/'.format(split)\n",
    "\n",
    "            pos_examples = self.read_folder(pos_dir)\n",
    "            neg_examples = self.read_folder(neg_dir)\n",
    "\n",
    "            pos_examples_tokenized = [(ids, 1) for ids in self.tokenize(pos_examples)]\n",
    "            neg_examples_tokenized = [(ids, 0) for ids in self.tokenize(neg_examples)]\n",
    "\n",
    "            self.data = pos_examples_tokenized + neg_examples_tokenized\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def read_folder(self, folder):\n",
    "        examples = []\n",
    "        for fname in os.listdir(folder)[:self.data_limit]:\n",
    "            with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "                examples.append(f.readline().strip())\n",
    "        return examples\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "\n",
    "        example_ids = []\n",
    "        misses = 0              # Count the number of tokens in our dataset which are not covered by glove -- i.e. percentage of unk tokens\n",
    "        total = 0\n",
    "        for example in examples:\n",
    "            tokens = word_tokenize(example)\n",
    "            ids = []\n",
    "            for tok in tokens:\n",
    "                if tok in word2id:\n",
    "                    ids.append(word2id[tok])\n",
    "                else:\n",
    "                    misses += 1\n",
    "                    ids.append(word2id['unk'])\n",
    "                total += 1\n",
    "\n",
    "            if len(ids) >= self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "            else:\n",
    "                ids = ids + [word2id['<pad>']]*(self.max_length - len(ids))\n",
    "            example_ids.append(torch.tensor(ids))\n",
    "        print('Missed {} out of {} words -- {:.2f}%'.format(misses, total, misses/total))\n",
    "        return example_ids\n",
    "\n",
    "    def generate_validation_split(self, ratio=0.8):\n",
    "\n",
    "        split_idx = int(ratio * len(self.data))\n",
    "\n",
    "        # Take a chunk of the processed data, and return it in order to initialize a validation dataset\n",
    "        validation_split = self.data[split_idx:]\n",
    "\n",
    "        #We'll remove this data from the training data to prevent leakage\n",
    "        self.data = self.data[:split_idx]\n",
    "\n",
    "        return validation_split\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our two models: the randomly initialized RandomModel and the GloveModel where we use the pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple classification model\n",
    "class RandomModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers, max_length=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_layer_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input).squeeze(1)\n",
    "        embedding = torch.sum(embedding, dim=1)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "\n",
    "# Define a Glove classification model\n",
    "class GloveModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embedding, hidden_dim, num_hidden_layers, max_length=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embedding))\n",
    "        self.hidden_layer_1 = nn.Linear(pretrained_embedding.shape[1] * max_length, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input).squeeze(1)\n",
    "        embedding = torch.sum(embedding, dim=1)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll define a new prediction method. It will take the output of the model and classify it as 0 if it's below the threshold (0.5) or 1 otherwise.\n",
    "\n",
    "We'll use this method to log our validation accuracy as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, valid_dataloader):\n",
    "\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_examples = len(valid_dataloader)\n",
    "\n",
    "    for x,y in valid_dataloader:\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        output = sigmoid(model(x))\n",
    "\n",
    "        if (output < 0.5 and y == 0) or (output >= 0.5 and y == 1):\n",
    "            total_correct += 1\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the training loop for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_classification(model, train_dataset, valid_dataset, epochs=100, batch_size=32, print_frequency=100):\n",
    "\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())            \n",
    "    \n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predict(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the training and validation datasets/dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MovieReviewDataset('../Homework 1/aclImdb/', 'train', word2id)\n",
    "validation_examples = train_dataset.generate_validation_split()\n",
    "print('Loaded {} train examples'.format(train_dataset.__len__()))\n",
    "\n",
    "valid_dataset = MovieReviewDataset(finalized_data=validation_examples, word2id=word2id)\n",
    "print('Loaded {} validation examples'.format(valid_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two code blocks, initialize a new RandomModel in one, and a new GloveModel in the other -- use train_classification() to train them. For each model, find a set of model parameters (i.e. play around with the number of hidden layers and the hidden layer size) and training parameters (epochs, batch size) which give you a good (>70) validation accuracy.\n",
    "\n",
    "Some tips:\n",
    "    1. Given your resources, first try and prioritize how many data examples you load. This is controlled by the data_limit value of the dataset.\n",
    "    2. For previous models, we've only trained for 1-3 epochs due to the large number of parameters when language modeling. You may need to train for a considerably longer time (>30-50 epochs) to get results\n",
    "    3. Performance is both a function of training time and the model itself. Keep an eye on the validation accuracy in case the model is overfitting (can be prevented by using more examples)\n",
    "    4. Right now, every hidden layer is the same dimension. Consider widening or narrowing some layers.\n",
    "\n",
    "Additionally, modify the training loop to collect validation set accuracies after each epoch (the predict method is already returning these values). For each model, plot the training loss and validation accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Initialize the RandomModel here.\n",
    "\n",
    "\n",
    "Your code here\n",
    "\n",
    "10 pts.\n",
    "'''\n",
    "\n",
    "random_model = RandomModel()\n",
    "train_classification(random_model, train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Initialize the GloveModel here.\n",
    "\n",
    "\n",
    "Your code here\n",
    "\n",
    "10 pts.\n",
    "'''\n",
    "\n",
    "glove_model = GloveModel()\n",
    "train_classification(glove_model, train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished tuning parameters, test the two models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = MovieReviewDataset('../Homework 1/aclImdb/', 'test', word2id)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Random model accuracy: ')\n",
    "predict(random_model, test_dataloader)\n",
    "\n",
    "print('Glove model accuracy: ')\n",
    "predict(glove_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Response Questions (20 pts.):\n",
    "1. Compare the performance of the Glove model vs the Random model. Refer to the validation accuracy curves and the test set results in your answer.\n",
    "2. Compare the training loop between the supervised and unsupervised models. What's different (outside of code features like predicting validation accuracy after each epoch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "_Your answer here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
