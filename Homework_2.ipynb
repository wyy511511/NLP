{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Homework 2: Language Models and Neural Networks\n",
    "#### CSCI 3832 Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your name and email here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this homework we're going to be looking at the bigram language model you've implemented in class, and extend it to trigrams.\n",
    "\n",
    "Instead of looking at the Bible, we'll re-visit the sentiment analysis problem from the previous homework. This dataset contains a split of unlabeled movie reviews. We'll train our language model using this unlabeled split (i.e. we'll pretrain our language model) and then we'll use this model as a starting off point for a neural classification model (i.e. finetuning), which we'll use to do sentiment classification. Finally, we'll replace our trained embeddings with Glove pretrained vectors, to see if we get any improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Neural Language Modeling\n",
    "\n",
    "We'll first load the unsupervised data. Set the data dir below to the directory you used for Homework 1, to prevent copying the data twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os, random, sys, matplotlib.pyplot as plt\n",
    "import torch, torch.nn as nn, numpy\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I admit, the great majority of films released before say 1933 are just not for me. Of the dozen or so \"major\" silents I have viewed, one I loved (The Crowd), and two were very good (The Last Command and City Lights, that latter Chaplin circa 1931).<br /><br />So I was apprehensive about this one, and humor is often difficult to appreciate (uh, enjoy) decades later. I did like the lead actors, but thought little of the film.<br /><br />One intriguing sequence. Early on, the guys are supposed to get \"de-loused\" and for about three minutes, fully dressed, do some schtick. In the background, perhaps three dozen men pass by, all naked, white and black (WWI ?), and for most, their butts, part or full backside, are shown. Was this an early variation of beefcake courtesy of Howard Hughes?\n"
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/a511/Desktop/cu23 spring/3832/aclImdb/'\n",
    "data_limit = 15000\n",
    "\n",
    "def read_folder(folder):\n",
    "    examples = []\n",
    "#     for fname in os.listdir(folder)[:data_limit]:\n",
    "    files = os.listdir(folder)\n",
    "    files.sort()\n",
    "    for fname in files[:data_limit]:\n",
    "        with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "            examples.append(f.readline().strip())\n",
    "    return examples\n",
    "\n",
    "unsup_examples = read_folder(os.path.join(data_dir, 'train/unsup/'))\n",
    "print(unsup_examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset also comes with a pre-made vocabulary, which we'll rely on for this section of the homework. We'll eventually convert our words to indices, so lets store the words in a dictionary, mapping each to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocabulary_file = os.path.join(data_dir, 'imdb.vocab')\n",
    "\n",
    "raw_vocabulary = []\n",
    "with open(vocabulary_file, 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        raw_vocabulary.append(line.strip())\n",
    "\n",
    "#Limit our vocabulary size to top 5k words\n",
    "raw_vocabulary = raw_vocabulary[:5000]\n",
    "\n",
    "# Add in our special tokens\n",
    "special_tokens = ['<s>', '</s>', '<unk>']\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "'''\n",
    "\n",
    "Your code here.\n",
    "\n",
    "Create the vocabulary dictionary by prepending the special tokens to the raw vocabulary, and enumerating them.\n",
    "\n",
    "10 pts.\n",
    "\n",
    "'''\n",
    "vocabulary_list = special_tokens + raw_vocabulary\n",
    "vocabulary = {word: index for index, word in enumerate(vocabulary_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(vocabulary, dict)\n",
    "assert len(vocabulary) == 5003\n",
    "assert vocabulary['<s>'] == 0\n",
    "assert vocabulary['significance'] == 5002\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# for example in unsup_examples:\n",
    "#     i = i+1\n",
    "#     if i > 10:\n",
    "#         break\n",
    "#     print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vocabulary['significance'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that we have a vocabulary, we can process the unsupervised examples we loaded earlier into actual training data our model can read.\n",
    "\n",
    "First, we'll tokenize the text normally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This block may take a while (<5 minutes) to run, but you only have to run it once, so make sure you don't modify the tokenized_examples list after it's completed.\n",
    "While you're writing your code, consider limiting unsup_examples to the first 5 examples as a smoke test before you run the loop over all examples\n",
    "\"\"\"\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_examples = []\n",
    "sos_id = vocabulary['<s>'] #start of sequence\n",
    "eos_id = vocabulary['</s>'] #end of sequence\n",
    "unk_id = vocabulary['<unk>']\n",
    "\n",
    "i = 0\n",
    "for example in unsup_examples:\n",
    "    i = i+1 \n",
    "    example_tokens = [token.lower() for token in word_tokenize(example)]\n",
    "    if i>3:\n",
    "        break\n",
    "\n",
    "    token_ids = [sos_id]\n",
    "    for token in example_tokens:\n",
    "#         print(token)\n",
    "        '''\n",
    "            Your code here.\n",
    "\n",
    "            The above loop iterates over the tokens in a single example. If a token is in our vocabulary, then add it to token_ids. If not, add the unknown token.\n",
    "\n",
    "            10 pts.\n",
    "            \n",
    "\n",
    "        '''\n",
    "#         token_id = vocabulary.get(token, unk_id)\n",
    "#         token_ids.append(token_id)\n",
    "        if token in vocabulary:\n",
    "            token_id = vocabulary[token]\n",
    "        else:\n",
    "            token_id = unk_id\n",
    "        token_ids.append(token_id)\n",
    "\n",
    "\n",
    "    token_ids.append(eos_id)\n",
    "    tokenized_examples.append(token_ids)\n",
    "# print(tokenized_examples[0])\n",
    "# print(len(tokenized_examples[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt = 0\n",
    "# for i in tokenized_examples[0]:\n",
    "#     if i == 2:\n",
    "#         continue\n",
    "#     cnt += 1\n",
    "# print(cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert len(tokenized_examples[0]) == 191\n",
    "assert tokenized_examples[0] == [0, 11, 940, 2, 3, 86, 2091, 6, 107, 618, 158, 134, 2, 25, 43, 22, 16, 71, 2, 6, 3, 2587, 42, 38, 2, 653, 2, 2, 11, 27, 2361, 2, 29, 11, 440, 2, 3, 2260, 2, 2, 4, 108, 70, 55, 51, 2, 3, 229, 4300, 4, 522, 2648, 2, 13, 1517, 3053, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 38, 11, 14, 2, 44, 12, 29, 2, 4, 468, 8, 388, 854, 7, 1103, 2, 2, 2, 345, 2, 2738, 294, 2, 11, 120, 39, 3, 471, 154, 2, 19, 201, 115, 6, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 29, 1731, 695, 2, 387, 21, 2, 3, 473, 25, 410, 7, 77, 2, 2, 2, 4, 16, 44, 291, 228, 2, 1300, 1796, 2, 84, 50, 2, 2, 10, 3, 943, 2, 367, 291, 2587, 334, 1320, 33, 2, 31, 1267, 2, 444, 4, 323, 2, 2, 49, 2, 2, 4, 16, 90, 2, 66, 2, 2, 173, 42, 377, 2, 2, 25, 598, 2, 14, 12, 34, 387, 2, 6, 2, 2, 6, 1951, 2, 49, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create our bigram data. We'll make use of the torch Dataset class. We only need to implement the `__getitem__` and `__len__` methods to make this work with other existing torch tools.\n",
    "\n",
    "For this dataset, for each example, iterate over its bigrams. If either one of the tokens is an unknown token, then do not save the bigram. Since we're using a small vocabulary, we'll have a lot of unknowns, and we don't want our model to always predict this token as the most likely next token.\n",
    "\n",
    "Note that with a normal sized vocabulary, training set, and model, you wouldn't necessarily want to do this -- unknowns would hopefully be relatively rare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class BigramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_data):\n",
    "\n",
    "        self.examples = []\n",
    "        for example in tokenized_data:              #Iterate over our dataset\n",
    "            for i in range(0,len(example) - 1):     #Iterate over the tokens of the example\n",
    "                '''\n",
    "                    Your code here.\n",
    "\n",
    "                    Bigrams should be a tuple of integers: (example[i], example[i+1])\n",
    "                    For each bigram, if either of example[i] or example[i+1] are unknown then do not add the bigram to our examples.\n",
    "\n",
    "                    10 pts.\n",
    "                '''\n",
    "                token1, token2 = example[i], example[i+1]\n",
    "                if token1 != unk_id and token2 != unk_id:\n",
    "                    bigram = (token1, token2)\n",
    "                    self.examples.append(bigram)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.examples)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll define the bigram model. This is similar to the one in class: the input is a single token, and the model outputs a probability over the whole vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_layer_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll train the model. This training loop is similar to the one shown in lecture, with a couple of differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n",
      "epoch: 0 batch: 1 loss: 0.017035591125488282\n",
      "### Epoch: 2 ###\n",
      "epoch: 1 batch: 1 loss: 0.017035534858703613\n",
      "### Epoch: 3 ###\n",
      "epoch: 2 batch: 1 loss: 0.017035411834716796\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "#Initialize our model -- keep it small with 1 hidden layer, and embedding sizes of 50\n",
    "bigram_model = BigramLM(len(vocabulary), 50, 50, 1)\n",
    "\n",
    "#Initialize our dataset using a subset of examples\n",
    "bigram_dataset = BigramDataset(tokenized_examples[:5000])\n",
    "\n",
    "criteria = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(bigram_model.parameters())            #AdamW is a popularly used optimizer\n",
    "# optimizer = torch.optim.SGD(bigram_model.parameters(), lr=0.5)    #Either of these optimizers could be used\n",
    "\n",
    "softmax = nn.Softmax(dim=2)\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "print_frequency = 1000\n",
    "\n",
    "#We'll create an instance of a torch dataloader to collate our data. This class handles batching and shuffling (should be done each epoch)\n",
    "train_dataloader = torch.utils.data.DataLoader(bigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for i in range(epochs):\n",
    "    print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "    bigram_model.train()\n",
    "    avg_loss = 0\n",
    "\n",
    "    for step, data in enumerate(train_dataloader):\n",
    "\n",
    "        x, y = data\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        model_output = bigram_model(x)\n",
    "        model_output_probabilities = softmax(model_output)\n",
    "\n",
    "        loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss += loss.item()\n",
    "        if step % print_frequency == 1:\n",
    "            print('epoch: {} batch: {} loss: {}'.format(\n",
    "                i,\n",
    "                step,\n",
    "                avg_loss / print_frequency\n",
    "            ))\n",
    "            avg_loss = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the loop above to train the model for at least 1 epoch.\n",
    "\n",
    "1. Modify the loop to keep track of the average loss before it's reset. Then, plot the losses using matplotlib below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Your code here.\n",
    "\n",
    "The average loss is reset after print_frequency iterations. Before it's set to 0, store it in a list that will persist throughout training.\n",
    "\n",
    "10 pts.\n",
    "\n",
    "'''\n",
    "def train_model(model, optimizer, dataset, batch_size, num_epochs, print_frequency):\n",
    "    loss_history = []\n",
    "    avg_loss = 0.0\n",
    "    count = 0\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_batch, target_batch = batch\n",
    "            log_probs = model(input_batch)\n",
    "            loss = F.cross_entropy(log_probs, target_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()\n",
    "            count += 1\n",
    "            if count % print_frequency == 0:\n",
    "                avg_loss /= print_frequency\n",
    "                loss_history.append(avg_loss)\n",
    "                print(f\"Epoch {epoch}, iter {count}: average loss = {avg_loss:.4f}\")\n",
    "                avg_loss = 0.0\n",
    "    return loss_history\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll modify our model and dataset to create a trigram language model. Here, the input will be two words rather than 1. The output will remain the same.\n",
    "\n",
    "Hint: since we have two inputs, we'll want to combine them in some way after we get their embeddings. An easy way to do this would be to concatenate the two embeddings together, creating a new vector of size 2*embedding_dim. This will be the input size of the first hidden dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TrigramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenized_data):\n",
    "        self.examples = []\n",
    "        for example in tokenized_data:\n",
    "            for i in range(len(example) - 2):\n",
    "#                 token1, token2, token3 = example[i], example[i+1], example[i+2]\n",
    "#                 if token1 != unk_id and token2 != unk_id and token3 != unk_id:\n",
    "#                     trigram = (token1, token2, token3)\n",
    "#                     self.examples.append(trigram)\n",
    "                token1, token2, token3 = example[i], example[i+1], example[i+2]\n",
    "                if token1 != unk_id and token2 != unk_id and token3!=unk_id:\n",
    "                    trigram = (token1, token2,token3)\n",
    "                    self.examples.append(trigram)\n",
    "\n",
    "\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        self.examples[idx]\n",
    "\n",
    "#         raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TrigramLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "\n",
    "        self.hidden_layer_1 = nn.Linear(2*embedding_dim, hidden_dim)  #Embedding_dim will have to be modified\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_1, input_2):\n",
    "        # Hint: we'll need to get an embedding for our second input somehow\n",
    "        # self.embedding_1 = self.embedding(input_1)\n",
    "        # self.embedding_2 =\n",
    "\n",
    "        # Hint: This might be one way to combine our embeddings\n",
    "        # self.embedding = torch.cat()\n",
    "        embedding_1 = self.embedding(input_1)\n",
    "        embedding_2 = self.embedding(input_2)\n",
    "\n",
    "        embeddings = torch.cat([embedding_1, embedding_2], dim=2)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embeddings))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "    \n",
    "\n",
    "#         for i in range(self.num_hidden_layers - 1):\n",
    "#             hidden = self.relu(self.hidden_layers[i](hidden))\n",
    "\n",
    "#         logits = self.output_layer(hidden)\n",
    "\n",
    "#         return logits\n",
    "\n",
    "#         raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_trigram(trigram_model, trigram_dataset):\n",
    "\n",
    "    criteria = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(trigram_model.parameters())\n",
    "    # optimizer = torch.optim.SGD(trigram_model.parameters(), lr=0.5)\n",
    "\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    epochs = 3\n",
    "    batch_size = 32\n",
    "    print_frequency = 1000\n",
    "\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(trigram_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        trigram_model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x1,x2, y = data\n",
    "\n",
    "            x1 = x1.unsqueeze(1)            \n",
    "            x2 = x2.unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = trigram_model(x1,x2)\n",
    "            model_output_probabilities = softmax(model_output)\n",
    "\n",
    "            loss = criteria(model_output_probabilities.squeeze(1), y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Epoch: 1 ###\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4g/9n6gzty13b758rhwfbvxwx980000gn/T/ipykernel_18446/2188505926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtrain_trigram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrigram_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrigram_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4g/9n6gzty13b758rhwfbvxwx980000gn/T/ipykernel_18446/4127757886.py\u001b[0m in \u001b[0;36mtrain_trigram\u001b[0;34m(trigram_model, trigram_dataset)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/nlp/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    181\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Complete the following code block by initializing the model/dataset and training for at least one epoch\n",
    "Hint: the models and dataset should be _extremely_ similar to the bigram model and dataset\n",
    "\n",
    "20 pts.\n",
    "\"\"\"\n",
    "# vocab_size = len(vocabulary)\n",
    "# embedding_dim = 50\n",
    "# hidden_dim = 50\n",
    "# num_hidden_layers = 1\n",
    "\n",
    "#trigram_model = TrigramLM(vocab_size, embedding_dim, hidden_dim, num_hidden_layers)\n",
    "trigram_model = TrigramLM(len(vocabulary), 50, 50, 1)\n",
    "\n",
    "\n",
    "trigram_dataset = TrigramDataset(tokenized_examples)\n",
    "\n",
    "\n",
    "train_trigram(trigram_model, trigram_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete this section, complete the Trigram model and dataset, and train the model for at least 1 epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Sentiment Analysis\n",
    "\n",
    "In this section we'll compare how a neural model similar to the one above performs on sentiment analysis. Then, we'll replace the embeddings with pretrained ones to see if that increases our performance. To make our life easier, we'll use the glove vocabulary for both models.\n",
    "\n",
    "You can download the embeddings from here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The glove vectors are distributed as a text file, with the word in the first column, and the embeddings in the remaining columns. We'll read in the embeddings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "glove_file = 'glove.6B.50d.txt'\n",
    "\n",
    "embeddings_dict = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        if i == 0:\n",
    "            print(line)\n",
    "        line = line.strip().split(' ')\n",
    "        word = line[0]\n",
    "        embed = numpy.asarray(line[1:], \"float\")\n",
    "\n",
    "        embeddings_dict[word] = embed\n",
    "\n",
    "print('Loaded {} words from glove'.format(len(embeddings_dict)))\n",
    "\n",
    "embedding_matrix = numpy.zeros((len(embeddings_dict)+1, 50)) #add 1 for padding\n",
    "\n",
    "word2id = {}\n",
    "for i, word in enumerate(embeddings_dict.keys()):\n",
    "\n",
    "    word2id[word] = i                                #Map each word to an index\n",
    "    embedding_matrix[i] = embeddings_dict[word]      #That index holds the Glove embedding in the embedding matrix\n",
    "\n",
    "# Our joint vocabulary for both models / sanity check to see if we've loaded it correctly:\n",
    "print(word2id['the'])\n",
    "print(embedding_matrix[word2id['the']])\n",
    "\n",
    "word2id['<pad>'] = embedding_matrix.shape[0] - 1\n",
    "print(embedding_matrix[word2id['<pad>']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create another dataset for our (now labeled) movie reviews. Do not change the max_length values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a classification dataset for the movie reviews\n",
    "\n",
    "\n",
    "class MovieReviewDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, directory=None, split=None, word2id=None, finalized_data=None, data_limit=250, max_length=256):\n",
    "        \"\"\"\n",
    "        :param directory: The location of aclImdb\n",
    "        :param split: Train or test\n",
    "        :param word2id: The generated glove word2id dictionary\n",
    "        :param finalized_data: We'll use this to initialize a validation set without reloading the data.\n",
    "        :param data_limit: Limiter on the number of examples we load\n",
    "        :param max_length: Maximum length of the sequence\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_limit = data_limit\n",
    "        self.max_length = max_length\n",
    "        self.word2id = word2id\n",
    "\n",
    "        if finalized_data:\n",
    "            self.data = finalized_data\n",
    "\n",
    "        else:\n",
    "\n",
    "            pos_dir = directory + '{}/pos/'.format(split)\n",
    "            neg_dir = directory + '{}/neg/'.format(split)\n",
    "\n",
    "            pos_examples = self.read_folder(pos_dir)\n",
    "            neg_examples = self.read_folder(neg_dir)\n",
    "\n",
    "            pos_examples_tokenized = [(ids, 1) for ids in self.tokenize(pos_examples)]\n",
    "            neg_examples_tokenized = [(ids, 0) for ids in self.tokenize(neg_examples)]\n",
    "\n",
    "            self.data = pos_examples_tokenized + neg_examples_tokenized\n",
    "\n",
    "            random.shuffle(self.data)\n",
    "\n",
    "    def read_folder(self, folder):\n",
    "        examples = []\n",
    "        for fname in os.listdir(folder)[:self.data_limit]:\n",
    "            with open(os.path.join(folder, fname), encoding='utf8') as f:\n",
    "                examples.append(f.readline().strip())\n",
    "        return examples\n",
    "\n",
    "    def tokenize(self, examples):\n",
    "\n",
    "        example_ids = []\n",
    "        misses = 0              # Count the number of tokens in our dataset which are not covered by glove -- i.e. percentage of unk tokens\n",
    "        total = 0\n",
    "        for example in examples:\n",
    "            tokens = word_tokenize(example)\n",
    "            ids = []\n",
    "            for tok in tokens:\n",
    "                if tok in word2id:\n",
    "                    ids.append(word2id[tok])\n",
    "                else:\n",
    "                    misses += 1\n",
    "                    ids.append(word2id['unk'])\n",
    "                total += 1\n",
    "\n",
    "            if len(ids) >= self.max_length:\n",
    "                ids = ids[:self.max_length]\n",
    "            else:\n",
    "                ids = ids + [word2id['<pad>']]*(self.max_length - len(ids))\n",
    "            example_ids.append(torch.tensor(ids))\n",
    "        print('Missed {} out of {} words -- {:.2f}%'.format(misses, total, misses/total))\n",
    "        return example_ids\n",
    "\n",
    "    def generate_validation_split(self, ratio=0.8):\n",
    "\n",
    "        split_idx = int(ratio * len(self.data))\n",
    "\n",
    "        # Take a chunk of the processed data, and return it in order to initialize a validation dataset\n",
    "        validation_split = self.data[split_idx:]\n",
    "\n",
    "        #We'll remove this data from the training data to prevent leakage\n",
    "        self.data = self.data[:split_idx]\n",
    "\n",
    "        return validation_split\n",
    "\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define our two models: the randomly initialized RandomModel and the GloveModel where we use the pretrained vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a simple classification model\n",
    "class RandomModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_hidden_layers, max_length=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_layer_1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "\n",
    "\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input).squeeze(1)\n",
    "        embedding = torch.sum(embedding, dim=1)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "        return output\n",
    "\n",
    "# Define a Glove classification model\n",
    "class GloveModel(nn.Module):\n",
    "\n",
    "    def __init__(self, pretrained_embedding, hidden_dim, num_hidden_layers, max_length=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(pretrained_embedding))\n",
    "        self.hidden_layer_1 = nn.Linear(pretrained_embedding.shape[1] * max_length, hidden_dim)\n",
    "        self.hidden_layers = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden_layers - 1)]\n",
    "        )\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        embedding = self.embedding(input).squeeze(1)\n",
    "        embedding = torch.sum(embedding, dim=1)\n",
    "\n",
    "        hidden = self.relu(self.hidden_layer_1(embedding))\n",
    "        for layer in self.hidden_layers:\n",
    "            hidden = self.relu(layer(hidden))\n",
    "\n",
    "        output = self.output_layer(hidden)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we'll define a new prediction method. It will take the output of the model and classify it as 0 if it's below the threshold (0.5) or 1 otherwise.\n",
    "\n",
    "We'll use this method to log our validation accuracy as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, valid_dataloader):\n",
    "\n",
    "    sigmoid = nn.Sigmoid()\n",
    "\n",
    "    total_correct = 0\n",
    "    total_examples = len(valid_dataloader)\n",
    "\n",
    "    for x,y in valid_dataloader:\n",
    "\n",
    "        x = x.unsqueeze(1)\n",
    "        output = sigmoid(model(x))\n",
    "\n",
    "        if (output < 0.5 and y == 0) or (output >= 0.5 and y == 1):\n",
    "            total_correct += 1\n",
    "\n",
    "    accuracy = total_correct / total_examples\n",
    "    print('accuracy: {}'.format(accuracy))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the training loop for these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_classification(model, train_dataset, valid_dataset, epochs=100, batch_size=32, print_frequency=100):\n",
    "\n",
    "    criteria = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters())            \n",
    "    \n",
    "\n",
    "    epochs = epochs\n",
    "    batch_size = batch_size\n",
    "    print_frequency = print_frequency\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        print('### Epoch: ' + str(i+1) + ' ###')\n",
    "\n",
    "        model.train()\n",
    "        avg_loss = 0\n",
    "\n",
    "        for step, data in enumerate(train_dataloader):\n",
    "\n",
    "            x, y = data\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            model_output = model(x)\n",
    "\n",
    "            loss = criteria(model_output.squeeze(1), y.float())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "            if step % print_frequency == 1:\n",
    "                print('epoch: {} batch: {} loss: {}'.format(\n",
    "                    i,\n",
    "                    step,\n",
    "                    avg_loss / print_frequency\n",
    "                ))\n",
    "                avg_loss = 0\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predict(model, valid_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the training and validation datasets/dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MovieReviewDataset('../Homework 1/aclImdb/', 'train', word2id)\n",
    "validation_examples = train_dataset.generate_validation_split()\n",
    "print('Loaded {} train examples'.format(train_dataset.__len__()))\n",
    "\n",
    "valid_dataset = MovieReviewDataset(finalized_data=validation_examples, word2id=word2id)\n",
    "print('Loaded {} validation examples'.format(valid_dataset.__len__()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following two code blocks, initialize a new RandomModel in one, and a new GloveModel in the other -- use train_classification() to train them. For each model, find a set of model parameters (i.e. play around with the number of hidden layers and the hidden layer size) and training parameters (epochs, batch size) which give you a good (>70) validation accuracy.\n",
    "\n",
    "Some tips:\n",
    "    1. Given your resources, first try and prioritize how many data examples you load. This is controlled by the data_limit value of the dataset.\n",
    "    2. For previous models, we've only trained for 1-3 epochs due to the large number of parameters when language modeling. You may need to train for a considerably longer time (>30-50 epochs) to get results\n",
    "    3. Performance is both a function of training time and the model itself. Keep an eye on the validation accuracy in case the model is overfitting (can be prevented by using more examples)\n",
    "    4. Right now, every hidden layer is the same dimension. Consider widening or narrowing some layers.\n",
    "\n",
    "Additionally, modify the training loop to collect validation set accuracies after each epoch (the predict method is already returning these values). For each model, plot the training loss and validation accuracy over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Initialize the RandomModel here.\n",
    "\n",
    "\n",
    "Your code here\n",
    "\n",
    "10 pts.\n",
    "'''\n",
    "\n",
    "random_model = RandomModel()\n",
    "train_classification(random_model, train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Initialize the GloveModel here.\n",
    "\n",
    "\n",
    "Your code here\n",
    "\n",
    "10 pts.\n",
    "'''\n",
    "\n",
    "glove_model = GloveModel()\n",
    "train_classification(glove_model, train_dataset, valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've finished tuning parameters, test the two models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_dataset = MovieReviewDataset('../Homework 1/aclImdb/', 'test', word2id)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "print('Random model accuracy: ')\n",
    "predict(random_model, test_dataloader)\n",
    "\n",
    "print('Glove model accuracy: ')\n",
    "predict(glove_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Free Response Questions (20 pts.):\n",
    "1. Compare the performance of the Glove model vs the Random model. Refer to the validation accuracy curves and the test set results in your answer.\n",
    "2. Compare the training loop between the supervised and unsupervised models. What's different (outside of code features like predicting validation accuracy after each epoch)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "_Your answer here._"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
